<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <title>AutoRob - Michigan Robotics 380/511 - Autonomous Mobile Manipulation Systems</title>
    <meta name="viewport" content="width=device-width; initial-scale=1.0; maximum-scale=1.0;">
    <meta name="description" content="AutoRob at Michigan (ROB 380, ROB 511, EECS 367) is an introduction to autonomous robotic mobile manipulation systems">
    <meta name="author" content="ocj">
    <meta property="og:image" content="https://autorob.org/images/um_fetch.jpg" />
    <meta property="og:title" content="AutoRob - Michigan Robotics 380/511 - Autonomous Mobile Manipulation Systems" />
    <meta property="og:description" content="AutoRob at Michigan (ROB 380, ROB 511, EECS 367) is an introduction to autonomous robotic mobile manipulation systems" />
    <meta property="og:url" content="http://autorob.org" />
    <meta property="og:type" content="article" />

    <link rel="shortcut icon" href="favicon.ico" type="image/x-icon">

    <!-- Combo with CSSNormalize, CSSGrids-Responsive, CSSForm, CSSTable, CSSList (v3.9.1) -->
    <link rel="stylesheet" href="resources/yahoo_cssbutton-min.css">
    <link rel="stylesheet" href="resources/yahoo_gallerycss-cssform-min.css">
    <link rel="stylesheet" href="resources/yahoo_cssgrids-responsive-min.css">
    <link rel="stylesheet" href="resources/yahoo_gallerycss-csslist-min.css">
    <link rel="stylesheet" href="resources/yahoo_cssnormalize-min.css">
    <link rel="stylesheet" href="resources/yahoo_gallerycss-csstable-min.css">

    <!-- Some custom styles to make things pretty. -->
    <link rel="stylesheet" type="text/css" href="resources/ui.css">

    <!-- RainbowJS Syntax Highlighting - Github Theme. 
         For more themes, go to https://github.com/ccampbell/rainbow/tree/master/themes -->
    <link rel="stylesheet" type="text/css" href="resources/rainbow_github.css">


    <!-- Modify header colors here to customize the look and feel of the site-->
    <style>
        
        .header {
            background: rgb(0, 39, 76);
         }
            .header h1 {
                color: white;
            }
             .header h2 {
                 font-weight:300;
                 margin:0;
                 color: rgb(116, 130, 230);
             }
    </style>

</head>

<body class='yui3-skin-sam'>

    <div id="headerMenu" class="yui3-menu yui3-menu-open yui3-menu-horizontal yui3-menu-fixed">
        <span class="yui3-menu-heading">AutoRob</span>
        <ul>
            <li class="yui3-menu-active"><a href="https://docs.google.com/document/d/1moNHZB0SFnVrHo1TK9rDlVRzyH72JZvrGjIqmVlospM/edit?usp=sharing">missive</a></li>    
            <li><a href="#schedule">schedule</a></li>
            <li><a href="#calendar">calendar</a></li>
            <li><a href="#staff">staff</a></li>
            <li><a href="https://us.prairielearn.com/pl/course_instance/172015/assessments">prairielearn</a></li>
            <li><a href="https://piazza.com/class/m5lhgfx5bpw74k">piazza</a></li>
            <li><a href="https://classroom.github.com/classrooms/155585996-autorob-wn25-classroom/assignments/kineval-stencil">kineval</a></li>
            <li><a href="https://docs.google.com/document/d/1GqghQKSpp5hekLmvouC6t9SccDH6m23nZonRg3x5onk/edit?usp=drive_link">git</a></li>   
            <!-- <li><a href="#faq">FAQ</a></li> <br> -->
            <br>
            <li>projects:</li>
            <li><a href="https://docs.google.com/document/d/1_bWSckopff3AUs9fZLBDUCGaelkRutceC4C1sfSjxoo/edit?usp=sharing">1</a></li>
            <li><a href="#assignment2" style="color:#444444";>2</a></li>
            <li><a href="#assignment3" style="color:#444444";>3</a></li>
            <li><a href="#assignment4" style="color:#444444";>4</a></li>
            <li><a href="#assignment5" style="color:#444444";>5</a></li>
            <li><a href="#assignment6" style="color:#444444";>6</a></li>
            <li><a href="#assignment7" style="color:#444444";>7</a></li>
<!-- unassigned
            <li><a href="#assignment0">0</a></li>
            <li><a href="#assignment1">1</a></li>
            <li><a href="#assignment2">2</a></li>
            <li><a href="#assignment3">3</a></li>
            <li><a href="#assignment4">4</a></li>
            <li><a href="#assignment5">5</a></li>
            <li><a href="#assignment6">6</a></li>
            <li><a href="#assignment7">7</a></li>
            <li><a href="#assignment0" style="color:#444444";>0</a></li>
            <li><a href="#assignment1" style="color:#444444";>1</a></li>
            <li><a href="#assignment2" style="color:#444444";>2</a></li>
            <li><a href="#assignment3" style="color:#444444";>3</a></li>
            <li><a href="#assignment4" style="color:#444444";>4</a></li>
            <li><a href="#assignment5" style="color:#444444";>5</a></li>
            <li><a href="#assignment6" style="color:#444444";>6</a></li>
            <li><a href="#assignment7" style="color:#444444";>7</a></li>
-->
        </ul>
    </div>



    <div class="header yui3-u-1">
        <br>
        <br>
        <h1 class="yui3-u-1">AutoRob</h1>
        <h2 class="yui3-u"style="color:white">Introduction to Autonomous Robotics</h2>
        <br>
        <h2 class="yui3-u">Michigan ROB 380 / EECS 367</h2>
        <br>
        <br>
        <h2 class="yui3-u"style="color:white">Mobile Manipulation Systems</h2>
        <br>
        <h2 class="yui3-u">Michigan ROB 511</h2>
        <br>
        <br>
        <h2 class="yui3-u">Winter 2025</h2>

     </div>
    <div class="content">
<p>
<center>
<img width=100% src="images/um_fetch.jpg">
</center>

<hr>

<hr>

<!--
<h2> Initial Action Items</h2>
<p>
Students enrolled in an AutoRob section (Robotics 320, Robotics 511, EECS 367) should do the following as soon as possible:
</p>

<ul>
<li><p>Complete the <a href="https://forms.gle/4jmoc7F9emf5pHAx8">Student Workflow Survey</a>
<li><p>Join the AutoRob Winter 2023 <a href="http://um-engin-autorob.slack.com">Slack workspace</a>
</ul>
-->


<h2>Introduction</h2>
<p>
AutoRob is an introduction to the computational foundations of autonomous robotics for programming modern mobile manipulation systems.  AutoRob covers fundamental concepts in autonomous robotics for the kinematic modeling of arbitrary open-chain articulated robots and algorithmic reasoning for autonomous path and motion planning, and brief coverage of dynamics and motion control.  These core concepts are contextualized through their instantiation in modern robot operating systems, such as <a href="http://ros.org">ROS</a> and <a href="http://lcm-proj.github.io/lcm/">LCM</a>.  AutoRob covers some of the fundamental concepts in computing, common to a <a href="https://eecs281staff.github.io/eecs281.org/">second semester data structures course</a>, in the context of robot reasoning, but without analysis of computational complexity.  The AutoRob <a href="#objectives">learning objectives</a> are geared to ensure students completing the course are fluent programmers capable of computational thought and can develop full-stack mobile manipulation software systems.
</p>

<p>
The AutoRob course can be thought of as an exploration into the foundation for reasoning and computation by autonomous robots capable of mobility and dexterity.  That is, given a robot as a machine with sensing, actuation, and computation, how do we build computational models, algorithms, software implementations that allow the robot to function autonomously, especially for <i>pick-and-place</i> tasks? Such computation involves functions for robots to perceive the world (as covered in Robotics 330, EECS 467, EECS 442, or EECS 542), make decisions towards achieving a given objective (this class as well as EECS 492), transforming action into motor commands (as covered in Robotics 310, Robotics 311, or EECS 367), and usably working with human users (as covered in Robotics 340).  Computationally, these functions form the basis of the <b>sense-plan-act</b> paradigm that defines the discipline of robotics as the study of <a href="https://dspace.mit.edu/bitstream/handle/1721.1/6569/AIM-1293.pdf">embodied intelligence</a>, as described by <a href="https://rodneybrooks.com">Brooks</a>.  Embodied intelligence allows for understanding and extending concepts essential for modern robotics, especially mobile manipulators such as the pictured <a href="http://fetchrobotics.com/research/">Fetch</a> robot.
</p>

<p>
AutoRob projects ground course concepts through implementation in <a href="https://en.wikipedia.org/wiki/JavaScript"</a>JavaScript</a>/<a href="https://en.wikipedia.org/wiki/HTML">HTML5</a> supported by the <a href="https://github.com/autorob/kineval-stencil">KinEval code stencil</a> (snapshot below from Mozilla Firefox), as well as tutorials for the <a href="https://ros.org">ROS</a> robot operating system and the <i><a href="https://ieeexplore.ieee.org/abstract/document/7354021">rosbridge</a></i> robot messaging protocol.  These projects will coverrobot middleware architectures and <a href=https://en.wikipedia.org/wiki/Publishâ€“subscribe_pattern">publish-subscribe messaging models</a>, graph search path planning (<a href="https://en.wikipedia.org/wiki/A*_search_algorithm">A* algorithm</a>), basic physical simulation (<a href="https://en.wikipedia.org/wiki/Classical_mechanics">Lagrangian dynamics</a>, <a href ="https://en.wikipedia.org/wiki/Numerical_integration">numerical integrators</a>), proportional-integral-derivative (<a href="https://en.wikipedia.org/wiki/PID_controller">PID</a>) control, forward kinematics (3D geometric <a href="https://en.wikipedia.org/wiki/Transformation_matrix">matrix transforms</a>, matrix stack composition of transforms, axis-angle rotation by <a href="https://en.wikipedia.org/wiki/Quaternion">quaternions</a>), inverse kinematics (<a href="https://en.wikipedia.org/wiki/Gradient_descent">gradient descent</a> optimization, geometric <a href="https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant">Jacobian</a>), and motion planning (simple <a href="https://en.wikipedia.org/wiki/Collision_detection">collision detection</a>, sampling-based <a href="https://en.wikipedia.org/wiki/Motion_planning">motion planning</a>).  Additional topics that could be covered include network socket programming, <a href="https://www.json.org/json-en.html">JSON</a> object parsing, potential field navigation, Cyclic Coordinate Descent, Newton-Euler dynamics, task and mission planning, Bayesian filtering, and Monte Carlo localization.  
</p>

<img width=100% src="images/kineval_fetch.png">


<!--
Caveats: robot will perform suboptimally based on rrt-planner, no self-collision checking, assume object is graspable and pickable, need grasp planner, no respect for joint motor and angle limits, translate between ROS
-->

<h3>KinEval code stencil and programming framework</h3>

<p>
AutoRob projects will use the KinEval code stencil that roughly follows conventions and structures from the <a href="http://ros.org">Robot Operating System (ROS)<a> and <a href="http://robotwebtools.org/">Robot Web Tools (RWT)</a> software frameworks, as widely used across robotics.  These conventions include the <a href="http://wiki.ros.org/urdf/Tutorials/Create%20your%20own%20urdf%20file">URDF</a> kinematic modeling format, <a href="http://wiki.ros.org/Topics">ROS topic</a> structure, and the <a href="https://github.com/RobotWebTools/rosbridge_suite/blob/develop/ROSBRIDGE_PROTOCOL.md"><i>rosbridge</i> protocol</a> for <a href="https://json.org/">JSON</a>-based messaging.

KinEval uses <a href="http://threejs.org/">threejs</a> for in-browser 3D rendering.  Projects also make use of the <a href="https://github.com/sloisel/numeric/">Numeric Javascript</a> external library (or <a href="https://mathjs.org">math.js</a>) for select matrix routines, although other math support libraries are being explored.  Auxiliary code examples and stencils will often use the <a href="http://jsfiddle.net/">jsfiddle</a> development environment. 
</p>

<table><tr><td style="background:#dddddd">
<img width=30% align="right" src="images/kineval_rosbridge.jpg">
<p>
<b>You will use an actual robot (at least once)!</b>
</p>

<p>
While AutoRob projects will be mostly in simulation, KinEval allows for your code to work with any robot that supports the <a href="https://github.com/RobotWebTools/rosbridge_suite/blob/develop/ROSBRIDGE_PROTOCOL.md"><i>rosbridge</i> protocol</a>, which includes any robot running ROS.  Given a URDF description, the code you produce for AutoRob will allow you to view and control the motion of any mobile manipulation robot with rigid links.  Your code will also be able to access the sensors and other software services of the robot for your continued work as a roboticist.
</p>

</td></tr></table>





<!-- moved to missive in google docs
<h3>Related Courses</h3>

<p>
In AutoRob, <a href="https://idioms.thefreedictionary.com/seeing+is+believing">coding is believing</a>. AutoRob is a computing-friendly pathway into robotics.  The course aims to provide broad exposure to autonomous robotics, but it does not cover the whole of robotics.  The scope of AutoRob is introductory kinematic modeling and reasoning.  
</p>

<p>
AutoRob is well-suited as preparation for a Major Design Experience, such as in <a href="https://www.youtube.com/playlist?list=PLDutmfAv2lfZc2DQVNHfNODWsokz85OJA">EECS 467</a> (Autonomous Robotics Laboratory).  In addition to AutoRob, EECS 467 and <a href="https://www.youtube.com/watch?v=9QUSexcsSZk">ROB 550</a> (Robotics Systems Laboratory) provide more extensive hands-on experience with a small set of real robotic platforms.  In contrast, AutoRob emphasizes the creation of a general kinematics and planning software stack in simulation, with interfaces to work with a diversity of real robots.
</p>

<p>
AutoRob complements courses covering computational perception, including autonomous navigation in Robotics 330 and EECS 467, probabilistic inference in ROB 530 Mobile Robotics, EECS 442 Computer Vision, and deep learning in <a href="https://web.eecs.umich.edu/~justincj/teaching/eecs498/WI2022/schedule.html">EECS 498 Deep Learning for Computer Vision</a> and <a href="https://deeprob.org">ROB 498 Deep Learning for Robot Perception</a>.  Robots can't work if they can't perceive their environment.
</p>

<p>
Inference methods covered in AutoRob are complementary to artificial intelligence courses, such as EECS 492 (Introduction to Artificial Intelligence), EECS 445 (Machine Learning), and EECS 595 (Natural Language Processing).
</p>


<p>
AutoRob is an excellent complement to courses in <a href="https://ece.engin.umich.edu/academics/course-information/course-descriptions/eecs-373/">embedded systems (EECS 373)</a> and <a href="https://www.eecs.umich.edu/courses/eecs473/">advanced topics in embedded system design (EECS 473)</a>, as well as courses in sensorimotor control (<a href="https://ece.engin.umich.edu/academics/course-information/course-descriptions/eecs-460/">EECS 460</a>, <a href="https://ece.engin.umich.edu/academics/course-information/course-descriptions/eecs-461/">EECS 461</a>, ME 461).   AutoRob and embedded systems perform synergistic functions necessary for a control loop on an autonomous robot. AutoRob is about reasoning from data provided by embedded systems on a robot to compute a control command that is then executed by an embedded system to produce motion.

</p>



<p>
  AutoRob is a computation-focused alternative to ME 567/ROB 510 (Robot Kinematics and Dynamics).  ME 567 is a more in-depth mathematical analysis of dynamics and control with extensive use of <a href="https://en.wikipedia.org/wiki/Denavit%E2%80%93Hartenberg_parameters">Denavit-Hartenberg parameters</a> for kinematics.  AutoRob has a greater emphasis on algorithmic methods for autonomous path and motion planning for high degree-of-freedom robots and mobile manipulators, along with use of quaternions and matrix stacks for kinematics.  
</p>

<p>
AutoRob is a viable precursor for courses that go deeper into theoretical abstractions in <a href="https://web.eecs.umich.edu/~dmitryb/courses/fall2018iar/index.html">algorithmic robotics (ROB 422)</a> and advanced topics in <a href="https://web.eecs.umich.edu/~dmitryb/courses/winter2019motionplanning/index.html">motion planning (ROB 520)</a>.
</p>

<p>
AutoRob can be taken in parallel with <a href="https://robotics.umich.edu/academic-program/courses/rob502-f20/">ROB 502 (Programming for Robotics)</a>.  AutoRob makes some accommodations for students with less programming experience than provided in common data structures courses, such as <a href="https://www.eecs.umich.edu/courses/eecs280/">EECS 280</a>.  However, for students new to computer programming, it is highly recommended to take AutoRob after ROB 502, EECS 280, or EECS 402.
</p>



<h3>Commitment to equal opportunity</h3>

<p>
We expected that all students to treat each other with respect.  As indicated in the <a href="https://bulletin.engin.umich.edu/rules/">General Standards of Conduct for Engineering Students</a>, this course is committed to a policy of equal opportunity for all persons and does not discriminate on the basis of race, color, national origin, age, marital status, sex, sexual orientation, gender identity, gender expression, disability, religion, height, weight, or veteran status. Please feel free to contact the course staff with any problem, concern, or suggestion. The University of Michigan <a href="https://oscr.umich.edu/statement">Statement of Student Rights and Responsibilities</a> provides greater detail about expected behavior and conflict resolution in our community of scholarship.
</p>

<h3>Accommodations for Students with Disabilities</h3>

<p>
If you believe an accommodation is needed for a disability, please let the course instructor know at your earliest convenience. Some aspects of this course, including the assignments, the in-class activities, and the way the course is usually taught, may be modified to facilitate your participation and progress. As soon as you make us aware of your needs, the course staff can work with the <a href="http://ssd.umich.edu">Services for Students with Disabilities</a> (SSD, 734-763-3000) office to help us determine appropriate academic accommodations. SSD typically recommends accommodations through a Verified Individualized Services and Accommodations (VISA) form. Any information you provide is private and confidential and will be treated as such. For special accommodations for any academic evaluation (exam, quiz, project), the course staff will need to receive the necessary paperwork issued from the SSD office by <i>January 26, 2024</i>.
</p>

<h3>Student mental health and well being</h3> 

<p>
The University of Michigan is committed to advancing the mental health and wellbeing of its students. If you or someone you know is feeling overwhelmed, depressed, and/or in need of support, services are available.  For help, please contact one of the many resources offered by the University that are committed helping students through challenging situations, including: <a href="https://medicine.umich.edu/dept/psychiatry/patient-care/psychiatric-emergency-service">U-M Psychiatric Emergency</a> (734-996-4747, 24-hour), <a href="https://caps.umich.edu/">Counseling and Psychological Services</a> (CAPS, 734-764-8312, 24-hour), and the <a href="https://care.engin.umich.edu/">C.A.R.E. Center</a> on North Campus. You may also consult the <a href="https://www.uhs.umich.edu">University Health Service</a> (UHS, 734-764-8320) as well as its services for alcohol or drug concerns.  

</p>
    -->

    <br>
    <hr>
    <br>
    
<h2 id="staff">Course Staff and Office Hours</h2>

<h3>Course Instructors</h3>
    
<table cellpadding=5 border=0  width="100%">
<tr>
  <td><img src="images/staff_imgs/profJenkins.jpg" alt="Staff Image" width="277" height="185"></td>
  <td><p><a href="http://web.eecs.umich.edu/~ocj">Chad Jenkins</a> 
    <br>
    ocj@umich
    <br>
    <i>GitHub:</i> ohseejay
    <br>
    Office Hours: by appointment
    <br>
    Office: Robotics 2236 
    </p> </td> 
</tr>

<tr>
  <td><img src="https://eecsnews.engin.umich.edu/wp-content/uploads/sites/2/2019/11/elizabeth_mamantov.jpg" alt="Staff Image" width="277" height="185"></td>
  <td><p><a href="">Elizabeth Goeddel</a> 
    <br>
    mamantov@umich
    <br>
    <i>GitHub:</i> emgoeddel
    <br>
    Office Hours: Tuesday 9:30 - 11:30 a.m.
    <br>
    Location: 2000 Robotics
    </p> </td> 
</tr>


<tr><td><h3>Graduate Student Instructors</h3></td></tr>

<tr>
  <td><img src="https://robotics102.org/um-f24/assets/images/staff/imadhav.jpg" alt="Staff Image" width="185" height="185"></td>
  <td>
    <p>Isaac Madhavaram
      <br>
      imadhav@umich
      <br>
      <i>GitHub:</i> imadhav21
      <br>
      Office Hours: Wednesday 2:00 - 4:00 p.m.
      <br>
      Location: 2000 Robotics
      </p>
</td>
</tr>


<tr>
  <td><img src="images/staff_imgs/Haoran.jpg" alt="" width="185" height="185"></td>
  <td><p>Haoran Zhang
  <br>
  haoranwh@umich
  <br>
  <i>GitHub:</i> HaoranZhangumich
  <br>
  Office Hours: Friday 4:30 - 6:30 p.m.
  <br>
  Location: 2000 Robotics
  </p></td>
</tr>

<tr>
  <td><img src="images/staff_imgs/nikhil_sridhar.jpg" alt="" width="185" height="185"></td>
  <td><p>Nikhil Sridhar
  <br>
  niksrid@umich
  <br>
  <i>GitHub:</i> theonlynick0430
  <br>
  Office Hours: Tuesday 3:30 - 5:30 p.m.
  <br>
  Location: 2320 Robotics
  </p></td>
</tr>



</table>

<h3 id="calendar">Office Hours Calendar</h3>

<iframe src="https://calendar.google.com/calendar/embed?src=c_30d306aa1a946368f87bb587b0ee206de03176dba48bb369f2b9c91efe259ba0%40group.calendar.google.com&ctz=America%2FNew_York" style="border: 0" width="800" height="600" frameborder="0" scrolling="no"></iframe>



<h2 id="meetings">Winter 2025 Course Structure</h2>

<p>
This semester, the AutoRob course is offered in a synchronous in-person format across a number of sections this semester: two undergraduate in-person sections (Robotics 380 and EECS 367) and an in-person graduate section (Robotics 511).  
</p>

<h3>Course Meetings: <br> Monday 4:30-7:20pm Eastern, Chrysler Building 133</h3>

<h3>Laboratory Sections <br> Friday 2:30-4:20pm Eastern, Chrysler Building 133</h3> 

<p> 
<a href="https://oh.eecs.umich.edu/admin/courses/rob380">The AutoRob office hours queue</a> hosted by EECS will be used to manage queueing for course office hours.



<h2>Discussion Services</h2>

<h3>Piazza</h3>
<p>
The <a href="https://piazza.com/class/m5lhgfx5bpw74k">AutoRob Course Piazza</a> workspace will be the primary service for course-related discussion threads and announcements. 
</p>

<!--
<h3>Slack</h3> 

<p>
The <i>AutoRob Winter 2024</i> workspace hosted by the <a href="umich.enterprise.slack.com">U-M Enterprise Slack</a> server will be used for optional course-related discussions.  <a href="https://en.wikipedia.org/wiki/Slack_(software)">Slack</a> is a cloud-hosted online discussion and collaboration system with functionality that resembles <a href="https://en.wikipedia.org/wiki/Internet_Relay_Chat">Internet Relay Chat (IRC)</a>.  Slack clients are available for most modern operating systems as well as through the web. Slack is <a href="https://safecomputing.umich.edu/dataguide/?q=node/257">FERPA</a> compliant.
</p>

<p>
<i>Actively engaging in course discussions is a great way to become a better roboticist.</i>
</p>
    -->


<!-- moved to missive in google docs    
<h2>Prerequisites</h2> 

<p>
This course has recommended prerequisites of "Linear Algebra" and "Data Structures and Algorithms", or permission from the instructor.
</p>

<p>
<i>Programming proficiency</i>: EECS 280 (Programming and Introductory Data Structures)  EECS 402 (Programming for Scientists and Engineers), ROB 502 (Programming for Robotics) or proficiency in data structures and algorithms should provide an adequate programming background for the projects in this course.  Interested students should consult with the course instructor if they have not taken EECS 281, EECS 402, ROB 502, or its equivalent, but have some other notable programming experience.  EECS 281 (Data Structures and Algorithms) is a required prerequisite for EECS 367, an Upper-Level CS elective.
</p>

<p>
<i>Mathematical proficiency</i>: Math 214, 217, 417, 419 or proficiency in linear algebra should provide an adequate mathematical background for the projects in this course.  Interested students should consult with the course instructor if they have not taken one of the listed courses or their equivalent, but have some other strong background with linear algebra.
</p>

<p>
<i>Recommended optional proficiency</i>: Differential equations, Computer graphics, Computer vision, Artificial intelligence
</p>

<p>
The instructor will do their best to cover the necessary prerequisite material, but no guarantees.  Linear algebra will be used extensively in relation to 3D geometric transforms and systems of linear equations.  Computer graphics is helpful for under-the-hood understanding of threejs.  Computer vision and AI share common concepts with this course.  Differential equations are used to cover modeling of motion dynamics and inverse kinematics, but not explicitly required.
</p>

<h2>Textbook</h2>

<p>
AutoRob is compatible with both the Spong et al. and Corke textbooks (listed below), although only one of these books is needed.  Depending on individual styles of learning, one textbook may be preferable over the other.  Spong et al. is the listed required textbook for AutoRob and is supplemented with additional handouts.  The Corke textbook provides broader coverage with an emphasis on intuitive explanation.  A pointer to the Lynch and Park textbook is provided for an alternative perspective on robot kinematics that goes deeper into spatial transforms in exponential coordinates.  Lynch and Park also provides some discussion and context for using ROS.  This semester, AutoRob will not officially support the Lynch and Park book, but will make every effort to work with students interested in using this text.
</p>

<p>
<a href="http://bcs.wiley.com/he-bcs/Books?action=index&itemId=0471649902&bcsId=2888"><b>Robot Modeling and Control</b></a>
<br>
Mark W. Spong, Seth Hutchinson, and M. Vidyasagar
<br>
Wiley, 2005 
<br>
<a href="http://www.amazon.com/Robot-Modeling-Control-Mark-Spong/dp/0471649902">Available at Amazon</a>

<p>
<h3><i>Alternate textbooks</i></h3>
<p>

<a href="http://www.springer.com/gp/book/9783319544120"><b>Robotics, Vision and Control: Fundamental Algorithms in MATLAB</b></a>
<br>
Peter Corke
<br>
Springer, 2011

<p>
<a href="http://modernrobotics.org"><b>Modern Robotics: Mechanics, Planning, and Control</b></a>
<br>
Kevin M. Lynch, Frank C. Park
<br>
Cambridge University Press, 2017



<p>
<h3><i>Optional texts</i></h3>
<p>

<a href="http://shop.oreilly.com/product/9780596517748.do"><b>JavaScript: The Good Parts</b></a>
<br>
Douglas Crockford
<br>
O'Reilly Media / Yahoo Press, 2008

<p>
<a href="http://mitpress.mit.edu/books/principles-robot-motion"><b>Principles of Robot Motion</b></a>
<br>
Howie Choset, Kevin M. Lynch, Seth Hutchinson, George A. Kantor, Wolfram Burgard, Lydia E. Kavraki, and Sebastian Thrun
<br>
MIT Press, 2005



<h2 id="grading">Projects and Grading</h2>

<p>
The AutoRob course will assign 7 projects (6 programming, 1 oral) and 4 quizzes across all sections of the course.  Students in the undergraduate sections will have 4 homework assignments.  Students in the graduate section will have advanced extension opportunities.
</p>  

<p>
Each project has been decomposed into a collection of features, each of which is worth a specified number of points.  AutoRob project features are graded as "checked" (completed) or "due" (incomplete). Prior to its due date, the grading status of each feature will be in the "pending" state.  In terms of workload, each project is expected to take approximately 4 hours of work on average (as a rough estimate from the <a href="https://docs.google.com/spreadsheets/d/1wTDb6Q0RgOiPrp4OpYW8iI849bQRGrGoy9jkF5cO7rI/edit#gid=0">EECS Workload Survey</a>).
</p>

<p>
Each quiz will consist of a short set of questions administered synchronously in class.  Quiz questions will be within the scope of previously covered lectures and graded projects.  Quiz questions will focus on project material and should be readily answerable given knowledge from correctly completing projects on time.  
</p>

<p>
Individual final grades are assigned based on the sum of points earned from coursework (detailed in subsections below). 
The timing and due dates for course projects and quizzes will be announced on an ongoing basis.  The official due date of a project is listed with its project description, such as for <a href="#assignment1">Assignment 1: Path Planning</a>.  Due dates listed in the <a href="#schedule">course schedule</a> are tentative.  All project work must be checked by the final grading deadline to receive credit.
</p>

<h3>ROB 380/EECS 367: Introduction to Autonomous Robotics</h3>

<p>
In the undergraduate sections, each fully completed project is weighted as 10 points.  The first three quizzes are weighted as 4 points.  The final quiz is weighted as 10 points.  Four homework assignments will be given to this section, with each weighted as 2 points.   
</p>

<p>
Based on this sum of points from completed coursework, an overall grade for the course is earned as follows: An "A" grade in the course is earned if graded coursework sums to 93 points or above; a "B" grade in the course is earned if graded coursework sums to 83 points or above; a "C" grade in the course is earned if graded coursework sums to 73 points or above.  The instructor reserves the option to assign appropriate course grades with plus or minus modifiers.
</p>

<p>
Students can access the course <a href="https://www.gradescope.com/courses/711547">gradescope</a> portal to access and complete homework assignments.
</p>

<p>
Students in the undergraduate section have the opportunity to earn 2 extra credit points through optional extensions of the course projects.  Extension points are limited to 3 <b>total</b> over the course of the semester, and are limited to the Base Offset transform feature for Forward Kinematics, URDF/FSM Dance Showcase for Project 4 and the IK100in60 extension for Inverse Kinematics. 
</p>

<h3>ROB 511: Mobile Manipulation Systems</h3>

<p>
In the graduate section, each fully completed project is weighted as 16 points.  The first three quizzes are weighted as 4 points.  The final quiz is weighted as 10 points.  Advanced extensions can be completed to earn up to 10 points.  Examples of advanced extensions include implementation of an LU solver for linear systems of equations, inverse kinematics by Cyclic Coordinate Descent, one additional motion planning algorithm, and maximal coordinate simulation of an articulated robot.  Advanced extensions are due by the course final grading deadline and do not need to be completed for the deadlines of each assignment.
</p>

<p>
Based on this sum of points from completed coursework, an overall grade for the course is earned as follows: An "A" grade in the course is earned if graded coursework sums to 130 points or above; a "B" grade in the course is earned if graded coursework sums to 115 points or above; a "C" grade in the course is earned if graded coursework sums to 100 points or above.  The instructor reserves the option to assign appropriate course grades with plus or minus modifiers.
</p>

<h3 id="project_rubric">Project Rubrics (tentative and subject to change)</h3>

<p>
The following project features are planned for AutoRob this semester.  Students are expected to complete all features.
</p>

<table cellpadding=5 border=0  width="100%">
<col align="center">
<col align="center">
<col align="center">
<tr bgcolor="#aaaaaa">     
        <th style="width:100px"><b><center>Points</center></b></th> 
        <th style="width:100px"><b><center>Sections</center></b></th>
        <th style="width:600px"><b><center>Feature</center></b></th>
</tr>
-->

    <!-- moved to missive in google docs
<tr bgcolor="#dddddd">     
<td></td><td></td><td>Assignment 1: 2D Path Planning</td>
</tr>

<tr><td>4</td><td>All</td><td> Heap implementation</td></tr>
<tr><td>6</td><td>All</td><td> A-star search</td></tr>
<tr><td>2</td><td>511</td><td> BFS</td></tr>
<tr><td>2</td><td>511</td><td> DFS</td></tr>
<tr><td>2</td><td>511</td><td> Greedy best-first</td></tr> 

<tr bgcolor="#dddddd">     
<td></td><td></td><td>Assignment 2: Pendularm</td>
</tr>

<tr><td>3</td><td>All</td><td> Euler integrator</td></tr>
<tr><td>3</td><td>All</td><td> Velocity Verlet integrator</td></tr>
<tr><td>4</td><td>All</td><td> PID control</td></tr>
<tr><td>2</td><td>511</td><td> Verlet integrator</td></tr>
<tr><td>2</td><td>511</td><td> RK4 integrator</td></tr>
<tr><td>2</td><td>511</td><td> Double pendulum</td></tr>

<tr bgcolor="#dddddd">     
<td></td><td></td><td>Assignment 3: Forward Kinematics</td>
</tr>

<tr><td>2</td><td>All</td><td> Core matrix routines</td></tr>
<tr><td>5</td><td>All</td><td> FK transforms</td></tr>
<tr><td>1</td><td>All</td><td> Joint selection/rendering</td></tr>
<tr><td>2</td><td>All</td><td> New robot definition</td></tr>
<tr><td>2</td><td>511</td><td> Base offset transform</td></tr>
<tr><td>4</td><td>511</td><td> Fetch <i>rosbridge</i> interface (due before final grading deadline)</td></tr>

<tr bgcolor="#dddddd">     
<td></td><td></td><td>Assignment 4: Dance Controller</td>
</tr>

<tr><td>6</td><td>All</td><td> Quaternion joint rotation</td></tr>
<tr><td>1</td><td>All</td><td> Interactive base control</td></tr>
<tr><td>1</td><td>All</td><td> Pose setpoint controller</td></tr>
<tr><td>2</td><td>All</td><td> Dance FSM</td></tr>
<tr><td>3</td><td>Ext</td><td> Joint limits</td></tr>
<tr><td>3</td><td>511</td><td> Prismatic joints</td></tr>

<tr bgcolor="#dddddd">     
<td></td><td></td><td>Assignment 5: Inverse Kinematics</td>
</tr>

<tr><td>6</td><td>All</td><td> Manipulator Jacobian</td></tr>
<tr><td>4</td><td>All</td><td> Gradient descent with Jacobian transpose</td></tr>
<tr><td>3</td><td>511</td><td> Jacobian pseudoinverse</td></tr>
<tr><td>3</td><td>511</td><td> Euler angle conversion</td></tr> 

<tr bgcolor="#dddddd">     
<td></td><td></td><td>Assignment 6: Motion Planning</td>
</tr>

<tr><td>4</td><td>All</td><td> 2D RRT-Connect</td></tr>
<tr><td>2</td><td>All</td><td> Robot Collision detection</td></tr>
<tr><td>4</td><td>All</td><td> Configuration space RRT-Connect</td></tr>
<tr><td>6</td><td>511</td><td> 2D RRT-Star</td></tr>

</table>





<h3>Project Submission and Regrading</h3>

<p>
<a href="https://en.wikipedia.org/wiki/Git">Git</a> repositories will be used for project implementation, version control, and submission for grading. The implementation of your project is submitted through an update to the <i>master</i> branch of your designated repository.  The course staff will support repositories that use <i>main</i> as their <a href="https://github.com/github/renaming">default git branch</a> to the best of our ability.  Updates to the master branch must be committed and pushed prior to the due date for each assignment for any consideration of full credit. Your implementation will be checked out and executed by the course staff. Through your repository, you will be notified by the course staff whether your implementation of assignment features is sufficient to receive credit.
</p>

<h3>Continuous Integration Project Grading</h3>

<p>
For the Winter 2024 semester, AutoRob will make use of "continuous integration grading" for student project implementations.  Grading for a particular project will occur once 3 days before the project deadline, and then continuously after the project deadline.  
</p>

<p>
The "CI grader" will automatically pull code from your repository, run tests for <b>all</b> assignments that are due to the current time, and push the results of grading back to your repository.  Please remember to not break the functionality of project features that are already working in your code.  The CI grader will run at regularly scheduled intervals each day.  The CI grader is new aspect of the AutoRob course, as an innovation for scaling the course.  Thus, grades automatically generated by the CI grader will be considered tentative and reviewable by the course staff.  Your feedback, understanding, and help to improve the CI grader will be greatly appreciated by the course staff.
</p>


<h3>Late Policy</h3>
<p>
<b>Do not submit assignments late.</b>  The course staff reserves the right to not grade late submissions. The course instructor reserves the right to decline late submissions and/or adjust partial credit on regraded assignments.
</p>
<p>
If granted by the course instructor, late submissions can be graded for partial credit, with the following guidelines.  Submissions pushed within two weeks past the project deadline will be graded for 80% credit.   Submissions pushed within four weeks of the project deadline will be graded for 60% credit.  Submissions pushed at any time before the semester project submission deadline (April 22, 2024) will be considered for 50% credit.  As a reminder, the course instructor reserves the right to decline late submissions and/or adjust partial credit on regraded assignments.
</p>

<h3>Regrading Policy</h3>
<p>
The regrading policy allows for submission and regrading of projects up through the final grading of projects, which will be April 22 for the Winter 2024 Semester.  This regrading policy will grant full credit for project submissions pushed to your repository before the corresponding project deadline.  If a feature of a graded project is returned as not completed (or "DUE"), your code can be updated for consideration at 80% credit.  This code update must be pushed to your repository within two weeks from when the originally graded project was returned.  Regrades of projects updated beyond this two week window can receive at most 60% credit. 
</p>

<h3>Completed Features Policy</h3>

<p>
All checked features must continue to function properly in your repository up through the final grading deadline (April 22, 2024).  Checked features that do not function properly for subsequent projects will be treated as a new submission and subject to the regrading policy.
</p>


<h3>Final Grading</h3>

<p>
All grading will be finalized on April 22, 2024.  Regrading of specific assignments can be done upon request during office hours.  No regrading will be done after grades are finalized.
<p>

<h3>Repositories</h3>

<p>
You are expected to use <a href="https://classroom.github.com/classrooms/155585996-autorob-wn24-classroom/assignments/kineval-stencil">kineval-stencil</a> as a <b>private</b> git repository for your project work this course through the AutoRob Winter 2024 GitHub Classroom.  Please do not use the <a href="https://github.com/autorob/kineval-stencil">public version</a> of the kineval-stencil repository, which is provided only for the spirit of open source.

<p>
There are many different tutorials for learning how to use git repositories.  For those new to <a href="https://en.wikipedia.org/wiki/Version_control">version control</a>, we realize git has a significant startup overhead and learning curve, but it is definitely worth the effort.  The first laboratory discussion in AutoRob will be dedicated to installing and using git.  The AutoRob course site also has its own basic <a href="#git_tutorial">quick start tutorial</a>.  The <a href="http://www.git-scm.com/book/en/v2">Pro Git book</a> provides an in-depth introduction to git and version control.  As different people often learn through different styles, the <a href="http://www-cs-students.stanford.edu/~blynn/gitmagic/">Git Magic tutorial</a> has also proved quite useful when a different perspective is needed.  <a href="http://rogerdudler.github.io/git-guide/">git: the simple guide</a> has often been a great and accessible quick start resource. 
</p>

<p>
We expect students to use these repositories for collaborative development as well as project submission. It is the responsibility of each student group to ensure their repository adheres to the Collaboration Policy and submission standards for each assignment. Submission standards and examples will be described for each assignment as needed.
</p>

<p>
<b>IMPORTANT:</b> Do not modify the directory structure in the <a href="https://github.com/autorob/kineval-stencil">KinEval</a> stencil.  For example, the file "home.html" should appear in the top level of your repository.  Repositories that do not follow this directory structure will not be graded.
</p>

<h3>Code Maintenance Policy and Branching</h3>

<p>
This section outlines expectations for maintenance of source code repositories used by students for submission of their work in this course.  Repositories that do not maintain these standards will not be graded at the discretion of the course staff.
</p>

<p>
Code submitted for projects in this course must reside in the <i>master</i> branch (or <i>main</i> branch) of your repository.  The directory structure provided in the KinEval code stencil must not be modified.  For example, the file "home.html" should appear in the top level directory of your repository.
</p>

<p>
The <i>master</i> branch must always maintain a working (or stable) version of your code for this course.  Code in the <i>master</i> branch can be analyzed at any time with respect to any assignment whose due date has passed.  Improperly functioning code on the <i>master</i> branch can affect the grading of an assignment (even after the assignment due date) up to the assignment of final grades.
</p>

<p>
The <i>master</i> branch must always be in compliance with the <a href="https://bulletin.engin.umich.edu/rules/">Michigan Honor Code</a> and <a href="https://github.com/autorob/autorob.github.io/blob/master/MichiganHonorLicense">Michigan Honor License</a>, as described below in the course Collaboration Policy.  To be considered for grading, a commit of code to your <i>master</i> branch must be signed with your name and the instructor name at the bottom of the file named LICENSE with an unmodified version of the Michigan Honor License.  Without a properly asserted license file, a code commit to your repository will be considered an incomplete submission and will be ineligible for grading.
</p>

<p>
If advanced extension features have been implemented and are ready for grading, such features must be listed in the file "advanced_extensions.html" in the top level directory of the <i>master</i> branch with usage instructions.  Advanced extension features not listed in this file may not be graded at the discretion of the course staff.
</p>

<h4>Branching</h4>
<p>
Students are encouraged to update their repository often with the help of branching.  Branching spawns a copy of code in your <i>master</i> branch into a new branch for development, and then merging integrates these changes back into <i>master</i> once they are complete.  For example, you can create an <i>Assignment-2</i> branch for your work on the second project while it is under development and any changes may be experimental, which will keep your <i>master</i> branch stable for grading.  Once you are confident in your implementation of the second project, you can merge your <i>Assignment-2</i> branch back into the <i>master</i> branch.  The <i>master</i> branch at this point will have working stable versions of the first and second projects, both of which will be eligible for grading.  Similarly, an <i>Assignment-3</i> branch can be created for the next project as you develop it, and then the <i>Assignment-3</i> branch can be merged into the <i>master</i> branch when ready for grading.  This configuration allows your work to be continually updated and built upon such that versions are tracked and grading interruptions are minimized.
</p>


<h3 id="collaboration_policy">Collaboration Policy</h3>

<p>
This collaboration policy covers all course material and assignments unless otherwise stated.  All submitted assignments for this course must adhere to the Michigan Honor License (the <a href="https://opensource.org/licenses/BSD-3-Clause">3-Clause BSD License</a> plus two academic integrity clauses).
</p>

<p>
Course material, concepts, and documentation may be discussed with anyone.  Discussion during quizzes is not allowed with anyone other than a member of the course staff.  Assignments may be discussed with the other students at the conceptual level.  Discussions may make use of a whiteboard or paper.  Discussions with others (or people outside of your assigned project group) cannot include writing or debugging code on a computer or collaborative analysis of source code. You may take notes away from these discussions, provided these notes do not include any source code.
</p>

<p>
The code for your implementation may not be shown to anyone outside of your assigned project group, including granting access to repositories or careless lack of protection. For example, you do not need to hide the screen of your computer from anyone, but you should not attempt to show anyone your code. When you are done using any robot device such that another group may use it, you must remove all code you have put onto the device. You may not share your code with others outside of your group. At any time, you may show others the implemented program running on a device or simulator, but you may not discuss specific debugging details about your code while doing so.
</p>

<p>
This policy applies to collaboration during the current semester <b>and</b> any past or future instantiations of this course.  Although course concepts are intended for general use, your implementation for this course must remain private after the completion of the course.  It is expressly prohibited to share any code previously written and graded for this course with students currently enrolled in this course.  Similarly, it is expressly prohibited for any students currently enrolled in this course to refer to any code previously written and graded for this course.
</p>

<p>
<b>IMPORTANT:</b> To acknowledge compliance with this collaboration policy, append your name to the file "LICENSE" in the main directory of your repository with the following text.  This appending action is your attestation of your compliance with the Michigan Honor License and the Michigan Honor Code statement: 
</p>
<pre style="color:black"> <code>
"I have neither given nor received unauthorized aid on this course project implementation, nor have I concealed any violations of the Honor Code."  
</code></pre>
<br>
<p>
This attestation of the honor code will be considered updated with the current date and time of each commit to your repository.  Repository commits that do not include this attestation of the honor code will not be graded at the discretion of the course instructor.
</p>

<p>
Should you fail to abide by this collaboration policy, you will receive no credit for this course.  The University of Michigan reserves the right to pursue any means necessary to ensure compliance. This includes, but is not limited to prosecution through The College of Engineering Honor Council, which can result in your suspension or expulsion from the University of Michigan.  Please refer to the <a href ="https://elc.engin.umich.edu/honor-council/">Engineering Honor Council</a> for additional information.
</p>

    -->

<h2 id="schedule"><a href="https://docs.google.com/spreadsheets/d/1I4H5AnzwMmDVplI_GTdUtpmsnkj1uohLkqVJC3lWWgI/edit?usp=sharing">Course Schedule</a> (tentative and subject to change)</h2>

    
 
<p>
Previously recorded slides and lecture recordings are provided for asynchronous and optional parts of the course, as well as preview versions of lectures.</p>
   <!--
<p>
Slides from this course borrow from and are indebted to many sources from around the web.  These sources include a number of excellent robotics courses at various universities.
</p>
-->
    
<p>
<iframe width=100% height=600 src="https://docs.google.com/spreadsheets/d/e/2PACX-1vTumNOAdaaR-8qqf4I5cZMbiXH4khrHEfzjI7SM2fbySjPvMBvm60QyJdg4xtpenQps1whmuu3-ngCr/pubhtml?gid=905905424&single=true"></iframe>
</p>


<br>
<br>
<br>
<hr>

<!-- 
<hr>
<hr>
<h1> Material beyond this point has not been assigned.  The descriptions below are unofficial and tentative.</h1>
<hr>
<hr>
<hr>
<br>
<br>
<br>
 -->

<!-- 
<h2 id="assignment0">Assignment 0: ROS Publisher/Subscriber</h2>  
<p>
<b><del>Due 11:59pm, Monday, January 16, 2023</del></b><br>
<b>Due 11:59pm, Wednesday, January 18, 2023</b>
</p>

<p>
This project is described in a <a href="https://docs.google.com/document/d/1M8SDxCB21wrKyidEvfKPGn3AgkjLOoShlsxi98F9lj4/edit?usp=sharing">separate document</a>.
</p>
 -->

<!--

<br>
<br>
<br>
<hr>
<hr>
<hr>
<h1> Material beyond this point has not been assigned.  The descriptions below are unofficial and tentative.</h1>
<hr>
<hr>
<hr>
<br>
<br>
<br>
-->


<h2 id="assignment1">Project 1: Path Planning</h2>  
<p>
<b>Due 3:00pm, Monday, January 27, 2025</b>
</p>

<p>Project 1 instructions can be found <a href="https://docs.google.com/document/d/1_bWSckopff3AUs9fZLBDUCGaelkRutceC4C1sfSjxoo/edit?usp=sharing">HERE.</a></p>


<!-- <hr>
<h1> Material beyond this point has not been assigned.  The descriptions below are unofficial and tentative.</h1>
<hr> -->

<!--
 -->
<br>
<br>
<br>
<hr>
<hr>
<hr>
<h1> Material beyond this point has not been assigned.  The descriptions below are unofficial and tentative.</h1>
<hr>
<hr>
<hr>
<br>
<br>
<br>





<hr>

<h2 id="assignment2">Assignment 2: Pendularm </h2>  
<p>
<b>Due 11:59pm, Friday, February 9, 2024</b>
</p>

<p>
Physical simulation is widely used across robotics to test robot controllers as well as generate training data for learned controllers.  Testing and training in simulation has many benefits, such as avoiding the risk of damaging a (likely expensive) robot and faster development of controllers.  The video below about the <a href="https://www.youtube.com/embed/VW-dOMBFj7o?si=A1x5PIUG9MlDi7AW">NVIDIA Isaac Sim</a> highlights many of the advantages and advancements towards narrowing the <a href="https://ieeexplore.ieee.org/abstract/document/9398246">sim-to-real</a> gap.  Simulation also allows for consideration of environments not readily available for testing, such as for interplanetary exploration.  We will now model and control our first robot, the Pendularm, to achieve an arbitrary desired setpoint state.
</p>

<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/VW-dOMBFj7o?si=A1x5PIUG9MlDi7AW" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
</center>

<!--
<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/vOssEL1xqNs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
</center>
-->

<p>
As an introduction to building your own robot simulator, your task is to implement a physical dynamics and servo controller for a simple 1 degree-of-freedom robot system.  This system is 1 DOF robot arm as a frictionless <a href="http://en.wikipedia.org/wiki/Pendulum">simple pendulum</a> with a rigid massless rod and idealized motor. A visualization of the Pendularm system is shown below.  Students in the graduate section will extend this system into a 2-link 2-DOF robot arm, as an actuated <a href="https://en.wikipedia.org/wiki/Double_pendulum">double pendulum</a>.
</p>

<p>
<center>
<a href="https://github.com/autorob/kineval-stencil/blob/master/project_pendularm/pendularm1.html"><img width=80% src="images/pendularm.png"></a>
</center>
</p>

<h3>Features Overview</h3>

<p>
  This assignment requires the following features to be implemented in the corresponding files in your repository:
</p>

<ul>
  <li> <p>Euler integrator in "project_pendularm/update_pendulum_state.js"</p></li>
  <li> <p>Velocity Verlet integrator in "project_pendularm/update_pendulum_state.js"</p></li>
  <li> <p>PID controller in "project_pendularm/update_pendulum_state.js"</p></li>
  <li> <p><i>[Grad section only]</i> Verlet integrator in "project_pendularm/update_pendulum_state.js"</p></li>
  <li> <p><i>[Grad section only]</i> Runge-Kutta 4 integrator in "project_pendularm/update_pendulum_state.js"</p></li>
  <li> <p><i>[Grad section only]</i> Double pendulum implementation in "project_pendularm/update_pendulum_state2.js"</p></li> 
</ul>

<p>
  Points distributions for these features can be found in the <a href="#project_rubric">project rubric section</a></td>. More details about each of these features and the implementation process are given below.
</p>

<!-- 
<!-- (uncomment marker begin 2)
-->

<h3>Implementation Instructions </h3>

<p>
The code stencil for the Pendularm assignment is available within the "project_pendularm" subdirectory of KinEval.
</p>

<p>
For physical simulation, you will implement several numerical integrators for a pendulum with parameters specified in the code stencil.  The numerical integrator will advance the state of the pendulum (angle and velocity) in time given the current acceleration, which your pendulum_acceleration function should compute using the pendulum equation of motion. Your code should update the angle and velocity in the pendulum object (pendulum.angle and pendulum.angle_dot) for the visualization to access. If implemented successfully, this ideal pendulum should oscillate about the vertical (where the angle is zero) and with an amplitude that preserves the initial height of the pendulum bob.
</p>


<p>
Students enrolled in the undergraduate section will implement numerical integrators for:
</p>

<ul>
<li><p><a href="http://en.wikipedia.org/wiki/Euler%27s_method">Euler's Method</a> </p></li>
<li><p><a href="http://en.wikipedia.org/wiki/Verlet_integration#Velocity_Verlet">Velocity Verlet</a></p></li> 
</ul>



<p>
For motion control, students in both undergraduate sections will implement a <a href="http://en.wikipedia.org/wiki/PID_controller">proportional-integral-derivative controller</a> to control the system's motor to a desired angle.  This PID controller should output control forces integrated into the system's dynamics.  You will need to tune the gains of the PID controller for stable and timely motion to the desired angle for a pendulum with parameters: length=2.0, mass=2.0, gravity=9.81. These default values are also provided directly in the init() function.
</p>

<p>
For user input, you should be able to: 
</p>

<ul>
<li><p>select the choice of integrator using the [0-4] keys (with the "none" integrator as a default),</p></li> 
<li><p>toggle the invocation of the servo controller with the 'c' or 'x' key (which is off by default),</p></li> 
<li><p>decrement and increment the desired angle of the 1 DOF servoed robot arm using the 'q' and 'e' keys, and</p> </li>
<li><p>(for the double pendulum) decrement and increment the desired angle of the second joint of the arm using the 'w' and 'r' keys, and</p> </li>
<li><p>momentarily disable the servo controller with 's' key (and allowing the arm to swing uncontrolled).</p></li>
</ul>

<h3> Graduate Section Requirement</h3>

<p>
Students enrolled in the graduate section will implement numerical integrators for:
</p>

<ul>
<li><p><a href="http://en.wikipedia.org/wiki/Euler%27s_method">Euler's Method</a> </p></li>
<li><p><a href="https://en.wikipedia.org/wiki/Verlet_integration#Verlet_integration_.28without_velocities.29">Verlet integration</a></p></li>
<li><p><a href="http://en.wikipedia.org/wiki/Verlet_integration#Velocity_Verlet">Velocity Verlet</a></p></li>
<li><p><a href="https://en.wikipedia.org/wiki/Runge%E2%80%93Kutta_methods#The_Runge.E2.80.93Kutta_method">Runge-Kutta 4</a></p></li>
</ul>

<p>
to simulate and control a single pendulum (in "update_pendulum_state.js"). Then, students in the graduate section will implement <b>one</b> of the above integrators for a double pendulum (in "update_pendulum_state2.js"). Any of the integrators may work as your choice for the double pendulum implementation, although the Runge-Kutta integrator is recommended. The double pendulum is allowed to have a smaller timestep than the single pendulum, within reasonable limits. A working visualization for the double pendularm will look similar to <a href="https://youtu.be/-8YH1JhklBw">this result video</a> by <a href="https://github.com/emgoeddel">mamantov</a>:
</p>

<!--
<center>
<iframe frameborder="1" width="560" height="315" src="www.youtube.com/embed/-8YH1JhklBw" title="YouTube video player"  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
</center>
-->

<h4> Advanced Extensions </h4>

<p>
Of the possible advanced extension points, one additional point for this assignment can be earned by generating a random desired setpoint state and using PID control to your Pendularm to this setpoint.  This code must randomly generate a new desired setpoint and resume PID control once the current setpoint is achieved.  <b> A setpoint is considered achieved if the current state matches the desired state up to 0.01 radians for 2 seconds.</b> The number of setpoints that can be achieved in 60 seconds must be maintained and reported in the user interface.  The invocation of this setpoint trial must be enabled a user pressing the "t" key in the user interface.
</p>

<p>
Of the possible advanced extension points, two additional points for this assignment can be earned by implementing a simulation of a planar <a href="https://en.wikipedia.org/wiki/Inverted_pendulum">cart pole system</a>.  This cartpole system should have joint limits on its prismatic joint and no motor forces applied to the rotational joint.  This cart pole implementation should be contained within the file "cartpole.html" under the "project_pendularm" directory.
</p>


<p>
Of the possible optional extension points, two additional points for this assignment can be earned by implementing a single pendulum simulator in maximal coordinates with a spring constraint enforced by <a href="https://en.wikipedia.org/wiki/Verlet_integration#Constraints">Gauss-Seidel optimization</a>.  This maximal coordinate pendulum implementation should be contained within the file "pendularm1_maximal.html" under the "project_pendularm" directory.  An additional point can be earned by extending this implementation to a cloth simulator in the file "cloth_pointmass.html". 
</p>

<p>
Of the possible advanced extension points, three additional points for this assignment can be earned by developing a <a href="https://en.wikipedia.org/wiki/Newton%E2%80%93Euler_equations">Newton-Euler simulation</a> for a single rigid body with a cube geometry without consideration of contact with other objects.  This maximal coordinate pendulum implementation should be contained within the file "rigid_body_sim.html" under the "project_pendularm" directory.
</p>


<p>
Of the possible advanced extension points, three additional points for this assignment can be earned by developing and implementing a maximal coordinate dynamical simulation of biped hopper with links as planar 2D rigid bodies capable of locomotion on a flat ground plane.  This maximal coordinate pendulum implementation should be contained within the file "hopper_planar.html" under the "project_pendularm" directory.
</p>


<p>
Of the possible optional extension points, one additional point for this assignment can be earned by implementing a plot visualization of the state and desired setpoint for the 1 DoF pendulum over a 20 second window (of simulation time) within the pendularm1.html user interface.  The Pendularm user interface must maintain at least the same usability as the provided pendularm1.html implementation.
</p>

<p>
Of the possible optional extension points, two additional points for this assignment can be earned by implementing a method for <a href="https://en.wikipedia.org/wiki/Model_predictive_control">model predictive control</a> within pendularm1.html for setpoint control.
</p>

<h3> Project Submission</h3>
<p>
For turning in your assignment, push your updated code to the <b>master</b> branch in your repository.  
</p>

<h3> Optional: Pendularm Setpoint Competition </h3>

<p>
Details will be provided for participation in the Pendularm Setpoint Competition.  Three additional points towards final grading will be awarded to the top performer in the Pendularm Setpoint Competition in each of the graduate and undergraduate sections.  One additional point will be granted to the second and third place performers in each of the graduate and undergradate sections.
</p>


<!-- 
<h3> Additional Notes</h3>
<p>
Students in the graduate section are strongly encouraged to extend their pendularm to a double pendulum.
</p>
(uncomment end 2) -->


<!-- <hr>
<h1> Material beyond this point has not been assigned.  The descriptions below are unofficial and tentative.</h1>
<hr> -->

<!-- 
<br>
<br>
<br>
<hr>
<hr>
<hr>
<h1> Material beyond this point has not been assigned.  The descriptions below are unofficial and tentative.</h1>
<hr>
<hr>
<hr>
<br>
<br>
<br>
 -->



<h2 id="assignment3">Assignment 3: Forward Kinematics</h2>  
<b>Due 11:59pm, Friday, Feb 23, 2024</b>
<p>
Forward kinematics (FK) forms the core of our ability to purposefully control the motion of a robot arm.  FK will provide us a general formulation for controlling any robot arm to reach a desired configuration and execute a desired trajectory.  Specifically, FK allows us to predict the spatial layout of the robot in our 3D world given a configuration of its joints.  For the purposes of grasping and dexterous tasks, FK gives us the critical ability to predict the location of the robot's gripper (also known as its "endeffector").  As shown in our <a href="http://www.iros2017.org/">IROS 2017</a> video below, such manipulation assumes a robot has already perceived its environment as a scene estimate of objects and their positions and orientations.  Given this scene estimate, a robot controller uses FK to evaluate and execute viable endeffector trajectories for grasping and manipulating an object.
</p>

<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/ry0mqY5I-04" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
</center>

<p>
In this assignment, you will render the forward kinematics of an arbitrary robot, given an arbitrary kinematic specification.  A collection of specifications for various robots is provided in the "robots" subdirectory of the KinEval code stencil.  These robots include the Rethink Robotics' Baxter and Sawyer robots, the Fetch mobile manipulator, and a variety of example test robots, as shown in the "Desired Results" section below.  To render the robot properly, you will compute matrix coordinate frame transforms for each link and joint of the robot based on the parameters of its hierarchy of joint configurations.  The computation of the matrix transform for each joint and link will allow KinEval's rendering support routines to properly display the full robot.  We will assume the joints will remain in their zero position, saving joint motion for the next assignment.  
</p>

<!-- 
<!-- (uncomment marker begin 3)
-->
<h3>Features Overview</h3>

<p>
  This assignment requires the following features to be implemented in the corresponding files in your repository:
</p>

<ul>
  <li> <p>[Optional, recommended] Fill in stencils for Just Starting Mode in "kineval/kineval_startingpoint.js"</p></li>
  <li> <p>Core matrix routines in "kineval/kineval_matrix.js"</p></li>
  <li> <p>FK transforms in "kineval/kineval_forward_kinematics.js"</p></li>
  <li> <p>Joint selection/rendering, based on the kinematic hierarchy in "kineval/kineval_robot_init_joints.js"</p></li>
  <!-- <li> <p>[Grad section only] Base offset transform in "kineval/kineval_forward_kinematics.js" </p></li> -->
  <li> <p><!-- [Grad section only]  -->New robot definition in your own file in the "robots" directory</p></li>
</ul>

<p>
  Points distributions for these features can be found in the <a href="#project_rubric">project rubric section</a></td>. More details about each of these features and the implementation process are given below.
</p>

<h3>Just Starting Mode</h3>

<p>
While previous assignments were implemented within self-contained subsections of the kineval_stencil repository, with this project, you will start working with the KinEval part of the stencil repository that also supports all future projects in the course.  This KinEval stencil allows for developing the core of a modeling and control computation stack (forward kinematics, inverse kinematics, and motion planning) in a modular fashion.
</p>

<p>
If you open "home.html" in this repository, you should see the disconnected pieces of a robot bouncing up and down in the default environment.  This initial mode is the "starting point" state of the stencil to help build familiarity with JavaScript/HTML5 and KinEval.
</p>
<img width=100% src="images/kineval_welcome.png">

<p>
Your (optional) first task is to make the bouncing robot in starting point mode responsive to keyboard commands.  Specifically, the robot pieces will move upward, stop/start jittering, move closer together, and further apart (although more is encouraged).  To do this, you will modify "kineval/kineval_startingpoint.js" at the sections marked with "STENCIL".  These sections also include code examples meant to be a quick (and very rough) introduction to JavaScript and homogeneous transforms for translation, assuming programming competency in another language.
</p>

<h3>Brief KinEval Stencil Overview</h3>
<p>
Within the KinEval stencil, the functions my_animate() and my_init() in "home.html" are the principal access points into the animation system.  my_animate() is particularly important as it will direct the invocation of functions we develop throughout the AutoRob course.  my_animate() and my_init() are called by the primary process that maintains the animation loop: kineval.animate() and kineval.init() within "kineval/kineval.js".
<br>
<br>
<b>IMPORTANT</b>: "kineval/kineval.js", kineval.animate(), kineval.init(), and any of the given robot descriptions should not be modified.
</p>

<p>
For Just Starting Mode, my_animate() will call startingPlaceholderAnimate() and startingPlaceholderInit(), defined in "kineval/kineval_startingpoint.js".  startingPlaceholderInit() contains JavaScript tutorial-by-example code that initializes variables for this project. startingPlaceholderAnimate() contains keyboard handlers and code to update the positioning of each body of the robot.  By modifying the proper variables at the locations labed "STENCIL", this code will update the transformation matrix for each geometry of the robot (stored in the ".xform" attribute) as a translation in the robot's world.  The ".xform" transform for each robot geometry is then used by kineval.robotDraw() to have the browser render the robot parts in the appropriate locations.
</p>


<h3>Forward Kinematics Files</h3>
<p>
Assuming proper completion of Just Starting Mode, you are now ready for implementation of robot forward kinematics.  The following files are included (within script tags) in "home.html".  You will modify these files for implementing FK:

<ul>
<li><p>"kineval/kineval_robot_init_joints.js" for initializing your robot object based on a given description object; modification of this file is required to add parent and child references to each link</p></li>
<li><p>"kineval/kineval_forward_kinematics.js" for implementing (a recursive) traversal over joints and links to compute transforms; traversal of forward kinematics is invoked from kineval.robotForwardKinematics() within my_animate() in home.html</p></li>
 <li><p>"kineval/kineval_matrix.js" for the implementation of your vector and matrix routines, such as for matrix multiplication, matrix generation, etc.</p></li>
</ul>
</p>

<h3>Core Matrix Routines</h3>

<p>
A good place to start with your FK implemetation is writing and testing the core matrix routines. "kineval/kineval_matrix.js" contains function stencils for all required linear algebra routines. You will need to uncomment and fill in all the functions provided in this file <b>except matrix_pesudoinverse and matrix_invert_affine</b>. You do <b>not</b> need to implement the pseudoinverse calculation for this assignment; you should leave the matrix_pseudoinverse function commented out. You can implement the affine inverse function, but it will not be used in or tested for this assignment.
</p>
<p>
It is good practice to test these functions before continuing with your FK implementation. Consider writing a collection of tests using example matrix and vector calculations from the lecture slides or other sources.
</p>

<h3> Robot Examples </h3>
<p>
Each file in the "robots" subdirectory contains code to create a robot data object.  This data object is initialized with the kinematic description of a robot (as well as some meta information and rendering geometries).  The kinematic description defines a hierarchical configuration of the robot's links and joints.  This description is a subset of the <a href="http://wiki.ros.org/urdf">Unified Robot Description Format (URDF)</a> converted into JSON format.  The basic features of URDF are described in <a href="http://wiki.ros.org/urdf/Tutorials/Create%20your%20own%20urdf%20file">this tutorial</a>.
</p>

<p>
<b>IMPORTANT (seriously):</b> The given robot description files should <b>NOT</b> be modified.  Code that requires modified robot description files will fail tests used for grading.  You are welcomed and encouraged to create new robot description files for additional testing.
</p>

<p>
The selection of different robot descriptions can occur directly in the URL for  "home.html".  As a default, the "home.html" in the KinEval stencil assumes the "mr2" robot description in "robots/robot_mr2.js".  Another robot description file can be selected directly in the URL by adding a robot parameter.  This parameter is segmented by a question mark and sets the robot file pointer to a given file local location, relative to "home.html".  For example, a URL with "home.html?robot=robots/robot_urdf_example.js" will use the URDF example description. Note that to see the selected robot model in your visualization, you will need to turn off Just Starting Mode and have your FK methods implemented; see the Invoking Forward Kinematics section for more details.
</p>

<h3> Initialization of Kinematic Hierarchy </h3>
<p>
In addition to the various existing initialization functions, you should extend the robot object to complete the kinematic hierarchy to specify the parent and children joints for each link.  This modification should be made in the kineval.initRobotJoints() function in "kineval/kineval_robot_init_joints.js".  The children array of a link should be defined for all links except the leaves of the kinematic tree, in which case the ".children" property should be left undefined.  For the KinEval user controls to work properly, the children array should be named the ".children" property of the link.
</p>

<p>
<b>Note</b>: KinEval refers to links and joints as strings, not pointers, within the robot object.  robot.joints (as well as robot.links) is an array of data objects that are indexed by strings.  Each of these objects stores relevant fields of information about the joint, such as its transform (".xform"), parent (".parent") and child (".child") in the kinematic hierarchy, local transform information (".origin"), etc.  As such, robot.joints['JointX'] refers to an object for a joint.  In contrast, robot.joints['JointX'].child refers to a string ('LinkX'), that can then be used to reference a link object (as robot.links['LinkX']).   Similarly, robot.links['LinkX'].parent refers to a joint as a string 'JointX' that can then then be used to reference a joint object in the robot.joints array.
</p>

<h3> Invoking Forward Kinematics</h3>

<p>
The function kineval.robotForwardKinematics() in "kineval/kineval_forward_kinematics.js" will be the main point of invocation for your FK implementation.  This function will need to call kineval.buildFKTransforms(), which is a function you will add to this file. kineval.buildFKTransforms() will update matrix transforms for the frame of each link and joint with respect to the global world coordinates.  The computed transform for each frame of the robot needs to be stored in the ".xform" field of each link or joint.  For a given link named 'LinkX', this xform field can be accessed as robot.links['LinkX'].xform.  For a given joint named 'JointX', this xform field can be accessed as robot.joints['JointX'].xform.  Once kineval.robotForwardKinematics() completes, the updated transforms for each frame are used by the function kineval.robotDraw() in the support code to render the robot.
</p>


<p>
A matrix stack recursion can be used to compute these global frames, starting from the base of the robot (specified as a string in robot.base).  This recursion should use the provided local translation and rotation parameters of each joint in relation to its parent link in its traversal of the hierarchy.  For a given joint 'JointX', these translation and rotation parameters are stored in the robot object as robot.joints['JointX'].origin.xyz and robot.joints['JointX'].origin.rpy, respectively.  The current global translation and rotation for the base of the robot (robot.base) in the world coordinate frame is stored in robot.origin.xyz and robot.origin.rpy, respectively.
</p>


<p>
To run your FK routine, you must toggle out of starting point mode.  This toggle can be done interactively within the GUI menu or by setting kineval.params.just_starting to false.  The code below in "home.html" controls starting point mode invocation, where a single line can be uncommented to use FK mode by default:
</p>

<pre style="color:black"> <code data-language="javascript">
// set to starting point mode is true as default
//   set to false once starting forward kinematics project
//kineval.params.just_starting = false;

if (kineval.params.just_starting == true) {
    startingPlaceholderAnimate();
    kineval.robotDraw();
    return;
}
</code></pre>

<p>
  <b>Note:</b> The stencil in "kineval/kineval_forward_kinematics.js" states that the user interface reuqires "robot_heading" and "robot_lateral", but these are for Assignment 4. You do not need these variables for this assignment.
</p>

<h3>Desired Results</h3>

<p>
The "robots/robot_mr2.js" example should produce the following:
</p>
<p> <center>
<img  width=90% src="images/fk_mr2_example.png">
</center> </p>


<p>
If implemented properly, the "robots/robot_urdf_example.js" example should produce the following rendering:
</p>
<p> <center>
<img  width=50% src="images/fk_urdf_example.png">
</center> </p>

<!--
 (uncomment end 3) -->

<p>
The "robots/robot_crawler.js" example should produce the following (shown with joint axes highlighted):
</p>
<p> <center>
<img  width=90% src="images/fk_crawler_example.png">
</center> </p>

<!--
<!-- (uncomment marker begin 4)
 -->

<h3> Interactive Hierarchy Traversal</h3>
<p>
Additionally, a correct implementation will be able to interactively traverse the kinematic hierarchically by changing the active joint.  The active joint has focus for user control, which will be used in the next assignment.  For now, we are using the active joint to ensure your kinematic hierarchy is correct.  You should be able to move up and down the kinematic hierarchy with the "k" and "j" keys, respectively.  You can also move between the children of a link using the "h" and "l" keys.
</p>

<h3> Orienting Joint Rendering Cylinders</h3>
<p>
The cylinders used as rendering geometries for joints are not aligned with joint axes by default.  The support code in KinEval will properly orient joint rendering cylinders.  To use this functionality, simply ensure that the vector_cross() function is correctly implemented in "kineval/kineval_matrix.js".  vector_cross() will be automatically detected and used to properly orient each joint rendering cylinder.
</p>

<!-- <h3> Undergraduate Advanced Extension </h3> -->

<h3> New Robot Description </h3>

<p>
Students also must create a new robot description file that is compatible with their KinEval forward kinematics routines. 

<ul>
<li><p>Students will create their new robot description by working as a pair with another student.</p></li>
<li><p>Each pair of students will submit their new robot description by presenting it to the class during the interactive session on March 6th for our robot showcase.</p></li>
 <li><p>In addition, students should push their description file to their assignment repository in a new file titled "robots/new_robot_description.js" before the start of the interactive session.</p></li>
  <li><p>Include the name or uniquename of your partner in a new "robot.partner_name" property of your robot description.</p></li>
</ul>

<p>
    Of the two possible points for this feature, one point is earned by showcasing your description to the class and the second point is earned based on your description being compatible witht the KinEval forward kinematics routines. 
</p>

<p>
<!-- Your newly created robot description should be placed in the "robots" directory with a filename with your username in the format "robot_uniqueid.js" if no external geometries are used for this robot (similar to the MR2 or Crawler robots).   -->
If external geometries are imported (similar to the Fetch and Baxter), the robot description should be in a new subdirectory with the robot's name.  The robot's name should also be used to name the URDF file, such as "robots/newrobotname/newrobotname.urdf.js".  It is requested that geometries for a new robot go into this directory within a "meshes" subdirectory, such as "robots/newrobotname/meshes".  Guidance can be provided during office hours about creating or converting URDF-based robot description files to KinEval-compliant JavaScript and importing Collada, STL, and Wavefront OBJ geometry files.
</p>

<!-- <p>
Students in the AutoRob Undergraduate Section can earn one additional point by creating a robot description for the RexArm 4-DOF robot arm, which can be used later in <a href="https://www.youtube.com/playlist?list=PLDutmfAv2lfZ9M0XyYfY4N8EwLJhy58G6">EECS 467 (Autonomous Robotics Laboratory)</a>.  <a href="robot_descriptions/REX Arm STLs.zip">Rexarm link geometries</a> are provided in <a href="https://en.wikipedia.org/wiki/STL_%28file_format%29">STL format</a>.   <a href="https://youtu.be/DTD93KXrfZw">RoBob Ross</a> is an example of a RexArm project from 467 in Winter 2017.  Below is a snapshot of a RexArm in KinEval created by mattdr:
</p> -->

<p> <center>
<img  width=90% src="images/fk_rexarm_example.png">
</center> <p>

<h3> Graduate Section Requirement</h3>
<p>
Students in the AutoRob Graduate Section must: 1) implement the assignment as described above to work with <b>all</b> given examples, which includes the Fetch, Baxter, and Sawyer robot descriptions, and 2) create a new robot description that works with KinEval.
</p> 

<!-- 
<h3> Real Robot Examples</h3>
<p>
Students may optionally implement their forward kinematics routines to work with <b>all</b> given examples, which includes the Fetch, Baxter, and Sawyer robot descriptions, with the following .
</p>
-->

<p>
The files "robots/fetch/fetch.urdf.js", "robots/baxter/baxter.urdf.js", and "robots/sawyer/sawyer.urdf.js" contain the robot data object for the Fetch, Baxter, and Sawyer kinematic descriptions.  The Fetch robot JavaScript file is converted from the <a href="https://github.com/fetchrobotics/fetch_ros/blob/indigo-devel/fetch_description/robots/fetch.urdf">Fetch URDF description</a> for ROS.  A similar process was also done for the <a href="https://github.com/RethinkRobotics/baxter_common/tree/master/baxter_description/urdf">Baxter URDF description</a>.
</p>

<p>
ROS uses a different default coordinate system than threejs, which needs to be taken into account in the FK computation for these three robots. ROS assumes that the Z, X, and Y axes correspond to the up, forward, and side directions, respectively.  In contrast, threejs assumes that the Y, Z, and X axes correspond to the up, forward, and side directions.  The variable robot.links_geom_imported will be set to true when geometries have been imported from ROS and set to false when geometries are defined completely within the robot description file. You will need to extend your FK implementation to compensate for the coordinate frame difference when this variable is set to true.
</p>

<!--
(uncomment marker end 4)
-->

<p>
A proper implementation for fetch.urdf.js description should produce the following (shown with joint axes highlighted):
</p>
<p> <center>
<img  width=90% src="images/fk_fetch_example.png">
</center> <p>
</p>

<p>
The "robots/sawyer/sawyer.urdf.js" example should produce the following:
</p>
<p> <center>
<img  width=90% src="images/fk_sawyer_example.png">
</center> </p>

<!--
<!-- (uncomment marker begin 5)
 -->

<!-- <p>
Your newly created robot description should be placed in the "robots" directory with a filename with your username in the format "robot_uniqueid.js" if no external geometries are used for this robot (similar to the MR2 or Crawler robots).  If external geometries are imported (similar to the Fetch and Baxter), the robot description should be in a new subdirectory with the robot's name.  The robot's name should also be used to name the URDF file, such as "robots/newrobotname/newrobotname.urdf.js".  It is requested that geometries for a new robot go into this directory within a "meshes" subdirectory, such as "robots/newrobotname/meshes".  Guidance can be provided during office hours about creating or converting URDF-based robot description files to KinEval-compliant JavaScript and importing Collada, STL, and Wavefront OBJ geometry files.
</p> -->

<p>
Students are highly encouraged to port URDF descriptions of real world robot platforms into their code.  Such examples of real world robot systems include the 
<a href="https://github.com/Kinovarobotics/kinova-movo/tree/master/movo_common/movo_description">Kinova Movo</a>, 
<a href="https://github.com/gkjohnson/nasa-urdf-robots/tree/master/val_description/model">NASA Valkyrie</a> and 
<a href="https://github.com/gkjohnson/nasa-urdf-robots/tree/master/r2_description">Robonaut 2</a>, 
<a href="https://github.com/team-vigir/vigir_atlas_common/tree/master/atlas_description">Boston Dynamics Atlas</a>, 
<a href="https://github.com/ros-industrial/universal_robot/tree/kinetic-devel/ur_description">Universal Robots UR10</a>, 
and
<a href="https://github.com/PR2/pr2_common/tree/indigo-devel/pr2_description">Willow Garage PR2</a>. 
</p>

<p>
The following KinEval-compatiable robot descriptions were created by students in past offerings of the AutoRob course.  These descriptions are available for your use:

<ul>
<li><p>Boston Dynamics <a href="robot_descriptions/atlas_yeyangf.zip">Atlas by yeyangf</a></p></li>
<li><p>Agility Robotics <a href="robot_descriptions/cassie_mungam.zip">Cassie by mungam</a></p></li>
<li><p>NASA <a href="robot_descriptions/robonaut2_nikhita.zip">Robonaut 2 by nikhita</a></p></li>
<li><p><a href="robot_descriptions/hsr_sajanptl.zip">Human Support Robot by sajanptl</a></p></li>
<li><p>KUKA <a href="robot_descriptions/kuka_lbr_iiwa_nmtvijay.zip">Lightweight Arm by nmtvijay</a></p></li>
<li><p><a href="robot_descriptions/r2d2_eeyan.js">R2D2-like robot by eeyan</a></p></li>
<li><p>Universal Robots <a href="robot_descriptions/UR10_chengyah.zip">UR10 by chengyah</a></p></li>
<li><p><a href="robot_descriptions/walle_sarahcc.zip">Wall-E-like robot by sarahcc</a></p></li>
</ul>
</p>

<h4> Advanced Extensions </h4>

<p>
Of the possible advanced extension points, two additional points for this assignment can be earned by generate a proper Denavit-Hartenberg table for the kinematics of the Fetch robot.  This table should be placed in the "robots/fetch" directory in the file "fetchDH.txt". 
</p>

<p>
Of the possible advanced extension points, three additional points for this assignment can be earned by implementing LU decomposition (with pivoting) routines for matrix inversion and solving linear systems.  These functions should be named "matrix_inverse" and "linear_solve" and placed within the file containing your matrix routines.
</p>

<p>
Of the possible advanced extension points, three additional points for this assignment can be earned by implementing rigid body transformations as dual quaternions (<a href-"http://www.xbdev.net/misc_demos/demos/dual_quaternions_beyond/paper.pdf">Kenwright 2012<a/>), in addition to the quaternion-based method described in class.  Use of dual quaternion transformations must be selectable from the KinEval user interface.
</p>

<p>
Of the possible advanced extension points, three additional points for this assignment can be earned by implementing rigid body transformations as products of exponentials.  Use of matrix exponential transformations must be selectable from the KinEval user interface.
</p>




<h3> Project Submission</h3>
<p>
For turning in your assignment, push your updated code to the <b>master</b> branch in your repository.  
</p>

<!--
(uncomment marker end 5)
-->

<!-- 
<hr>
<h1> Material beyond this point has not been assigned.  The descriptions below are unofficial and tentative.</h1>
<hr> 
-->


<p>

<h2 id="assignment4">Assignment 4: Robot FSM Dance Contest </h2>  
<b>Due 4:00pm, Monday, March 11, 2024</b>
<p>

<p>
Executing choreographed motion is the most common use of current robots.  Robot choreography is predominantly expressed as a sequence of setpoints (or desired states) for the robot to achieve in its motion execution.  This form of robot control can be found among a variety of scenarios, such as robot dancing (video below), GPS navigation of autonomous drones, and automated manufacturing.  General to these robot choreography scenarios is a given setpoint controller (such as our PID controller from Pendularm) and a sequence controller (which we will now create).
</p>

<!-- https://www.youtube.com/embed/n8-SSwKMGnY -->
<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/fn3KWM1kuAw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
</center>

<p>
For this assignment, you will build your own robot choreography system.  This choreography system will enable a robot to execute a dance routine by adding motor rotation to its joints and creating a Finite State Machine (FSM) controller over pose setpoints.  Your FK implementation will be extended to consider angular rotation about each joint axis using quaternions for axis-angle rotation.  The positioning of each joint with respect to a given pose setpoint will be controlled by a simple P servo implementation (based on the Pendularm assignment).  You will implement an FSM controller to update the current pose setpoint based on the robot's current state and predetermined sequence of setpoints.  For a single robot, you will choreograph a dance for the robot by creating an FSM with your design of pose setpoints and an execution sequence.
</p>

<p>
This controller for the "mr2" example robot was a poor attempt at <a href="https://www.youtube.com/watch?v=YxvBPH4sArQ&feature=youtu.be&t=107">robot Saturday Night Fever</a> (please do better):
</p>

<a href="asgn4_joint_rotation.png"><img width=100% src="images/asgn4_joint_rotation_small.png"></a>

<p>
This <a href="https://www.youtube.com/embed/WyQ9aoB3bpI">updated dance</a> controller for the Fetch robot is a bit better, but still very far from optimal:
</p>

<center>
<iframe width="420" height="315" src="https://www.youtube.com/embed/WyQ9aoB3bpI" frameborder="0" allowfullscreen></iframe>
</center>


<!-- 
<!-- (uncomment marker begin 6)
-->
<h3>Features Overview</h3>

<p>
  This assignment requires the following features to be implemented in the corresponding files in your repository:
</p>

<ul>
  <li> <p>Quaternion joint rotation in "kineval/kineval_quaternion.js" (for quaternion functions) and "kineval/kineval_forward_kinematics" (to add axis-angle joint rotation to existing kinematic traversal)</p></li>
  <li> <p>Interactive base control vectors in "kineval/kineval_forward_kinematics.js"</p></li>
  <li> <p>Pose setpoint controller in "kineval/kineval_servo_control.js"</p></li>
  <li> <p>Dance FSM in "kineval/kineval_servo_control.js" (FSM controller) and "home.html" (dance setpoint initialization)</p></li>
  <li> <p>[Grad section only] Joint limit enforcement in "kineval/kineval_controls.js"</p></li> 
  <li> <p>[Grad section only] Prismatic joint implementation in "kineval/kineval_forward_kinematics.js"</p></li> 
  <li> <p>[Grad section only] Fetch rosbridge interface</p></li> 
</ul>

<p>
  Points distributions for these features can be found in the <a href="#project_rubric">project rubric section</a></td>. More details about each of these features and the implementation process are given below.
</p>

<!--
<h3>Relevant Files</h3>
<p>
The following files are included (within script tags) in your "home.html".  You will modify these files for implementing axis-angle rotation, the pose setpoint controller, and the FSM controller:

<ul>
<li><p>"kineval/kineval_quaternion.js" for your implementation of quaternions for axis-angle rotation in 3D</p></li>
<li><p>"kineval/kineval_forward_kinematics.js" to augment your existing kinematic traversal to account for axis-angle joint rotation</p></li>
<li><p>"kineval/kineval_controls.js" includes function kineval.applyControls() to apply a control update to the robot's base and angle of each joint, as well as updating the camera position; this update just does an addition and does not consider a physical model of dynamics</p></li>
<li><p>"kineval/kineval_servo_control.js" for your implementation of a P servo controller and an FSM pose sequencer</p></li>
</ul>
</p>
-->

<h3>Joint Axis Rotation and Interactive Joint Control</h3>

<p>
Going beyond the joint properties you worked with in Assignment 3, each joint of the robot now needs several additional properties for joint rotation and control.  These joint properties for the current angle rotation (".angle"), applied control (".control"), and servo parameters (".servo") have already been created within the function kineval.initRobotJoints().  The joint's angle will be used to calculate a rotation about the joint's (normal) axis of rotation vector, specified in the ".axis" field.  To complete an implementation of 3D rotation due to joint movement, you will need to first implement basic quaternion functions in "kineval/kineval_quaternion.js" then extend your FK implementation in "kineval/kineval_forward_kinematics.js" to account for the additional rotations.
</p>

<p>
If joint axis rotation is implemented correctly, you should be able to use the 'u' and 'i' keys to move the currently active joint.  These keys respectively decrement and increment the ".control" field of the active joint.  Through the function kineval.applyControls(), this control value effectively adds an angular displacement to the joint angle.
</p>

<h3>Interactive Base Movement Controls</h3>

<p>
The user interface also enables controlling the global position and orientation of the robot base.  In addition to joint updates, the system update function kineval.applyControls() also updates the base state (in robot.origin) with respect to its controls (specified in robot.controls).  With the support function kineval.handleUserInput(), the 'wasd' keys are purposed to move the robot on the ground plane, with 'q' and 'e' keys for lateral base movement.  In order for these keys to behave properly, you will need to add code to update variables that store the heading and lateral directions of the robot base: robot_heading and robot_lateral.  These vectors need to be computed within your FK implementation in "kineval/kineval_forward_kinematics.js" and stored as global variables. They express the directions of the robot base's z-axis and x-axis in the global frame, respectively. Each of these variables should be a homogeneous 3D vector stored as a 2D array.
</p>

<p>
If robot_heading and robot_lateral are implemented properly, the robot should now be interactively controllable in the ground plane using the keys described in the previous paragraph.
</p>

<h3>Pose Setpoint Controller</h3>

<p>
Once joint axis rotation is implemented, you will implement a proportional setpoint controller for the robot joints in function kineval.robotArmControllerSetpoint() within "kineval/kineval_servo_control.js".  The desired angle for a joint 'JointX' is stored in kineval.params.setpoint_target['JointX'] as a scalar by the FSM controller or keyboard input. The setpoint controller should take this desired angle, the joint's current angle (".angle"), and servo gains (specified in the ".servo" object) to set the control (".control") for each joint.   All of these joint object properties are initialized in the function kineval.initRobotJoints() in "kineval/kineval_robot_init_joints.js". Note that the "servo.d_gain" is not used in this assignment; it is for advanced extensions.
</p>

<p>
Once you have implemented the control function described above, you can enable the conroller by either holding down the 'o' key or selecting 'persist_pd' from the UI.  With the controller enabled, the robot will attempt to reach the current setpoint. One setpoint is provided with the stencil code: the zero pose, where all joint angles are zero. Pressing the '0' key sets the current setpoint to the zero setpoint.
</p>

<p>
Besides the zero setpoint, up to 9 other arbitrary pose setpoints can be stored by KinEval (in kineval.setpoints) for pose control. You can edit kineval.setpoints in your code for testing and/or for the FSM controller (see below), but the current robot pose can also be interactively stored into the setpoint list by pressing "Shift+number_key" (e.g., "Shift+1" would store the current robot pose as setpoint 1). You can then select any of the stored setpoints to be the current control target by pressing one of the non-zero number keys [1-9] that corresponds to a previously-stored setpoint.  At any time, the currently stored setpoints can be output to the console as JavaScript code using the JSON.stringify function for the setpoint object: "JSON.stringify(kineval.setpoints);".  Once you have found the setpoints needed to implement your desired dance, this setpoint array can be included in your code as part of your dance controller.
</p>

<p>
Since you will need to implement your setpoint controller before your FSM controller, for additional testing of your setpoint controller, a "clock movement" FSM controller has been provided as the function setpointClockMovement() in "kineval/kineval_servo_control.js".  This function can be invoked by holding down the 'c' key or from the UI.  This controller goes well with <a href="https://www.youtube.com/watch?v=_JPa3BNi6l4"> this song</a>.
</p>



<h3>FSM Controller</h3>

<p>
  Once your pose setpoint controller is working, an FSM controller should be implemented in the function kineval.setpointDanceSequence() in "kineval/kineval_servo_control.js".  The reference implementation switches between the pose setpoints in kineval.setpoints based on two additional pieces of data: an array of indices (kineval.params.dance_sequence_index) and the current pose index (kineval.params.dance_pose_index). kineval.params.dance_sequence_index will tell your FSM the order in which the setpoints in kineval.setpoints should be selected to be the control target. Note that using this convention allows you to easily select the same setpoint multiple times to produce repetition in your dance. kineval.params.dance_pose_index is used to keep track of the current index within the dance pose sequence.
</p>

<p>
If this recommended variable convention is not used, the following line in "kineval/kineval_userinput.js" will require modification:
</p>

<pre style="color:black"><code data-language="javascript">
if (kineval.params.update_pd_dance)
    textbar.innerHTML += "executing dance routine, pose " + kineval.params.dance_pose_index + " of " + kineval.params.dance_sequence_index.length;
</code></pre>

<p>
To complete your dance controller, choreograph a dance by initializing kineval.setpoints with the poses for your dance and kineval.params.dance_sequence_index with the pose ordering. You should initialize these data structures within the my_init() function in "home.html". Once you have the poses and sequence for your dance initialized, when you select both "persist_pd" and "update_pd_dance" in the UI, you should see the robot move through the setpoints of your dance.
</p>

<h3> Graduate Section Requirements</h3>
<p>
Students in the graduate section of AutoRob must implement the assignment as described above for the Fetch and Baxter robots with two additional requirements: 1) proper implementation of all joint types in the robot descriptions and 2) proper enforcement of joint limits for the robot descriptions. and 3) integration (via <a href="http://wiki.ros.org/rosbridge_suite">rosbridge</a>) of their code with ROS or a <a href="http://docs.fetchrobotics.com/gazebo.html">Gazebo simulation of the Fetch</a>.

<p>
The urdf.js files for these robots, included in the provided code stencil, contain joints with with various types that correspond to different types of motion:
</p>

<p>
<ul>
<li><p>continuous: rotation about the joint axis with no joint limits</p></li>
<li><p>revolute: rotation about the joint axis with joint limits</p></li>
<li><p>prismatic: translation along the joint axis with joint limits</p></li>
<li><p>fixed: no motion of the joint</p></li>
</ul>
</p>

<p>
Joints are considered to be continuous as the default.  Joints with undefined motion types must be treated as continuous joints. The graduate section features for this assignment will be complete when your implementation correctly handles the direction of motion (rotation or translation) and limits of all of the above types of joints.
</p>


<p>
<b> <i>rosbridge</i> </b> allows your code can interface with any robot (or simulated robot) running rosbridge/ROS using the function kineval.rosbridge() in "kineval/kineval_rosbridge.js".  This code requires that the rosbridge_server package is running in a ROS run-time environment and listening on a websocket port, such as for ws://fetch7:9090.  If your FK implementation is working properly, the model of your robot in the browser will update along with the motion of the robot based on the topic subscription and callback.  This functionality works seamlessly between real and simulated robots.  Although this will not be done for this class, to control the robot arm, a rosbridge publisher must be written to update the ROS topic "/arm_controller/follow_joint_trajectory/goal" with a message of type "control_msgs/FollowJointTrajectoryActionGoal".
</p>

<p>
    Machines running rosbridge, ROS, and Gazebo for the Fetch will be available during special sessions of the class.  Students are encouraged to install and run the Fetch simulator on their own machines based on <a href="http://docs.fetchrobotics.com/gazebo.html">this tutorial</a>.
</p>

<h4> Robot FSM Dance Showcase </h4>

<p>
Students in the graduate section are required to present in the Robot Dance FSM Showcase in class on March 11.  Students in the undergraduate section can earn one additional extension point by presenting in this showcase.  Information for showcase presentations will be communicated through the class discussion channels.
</p>

<h4> Advanced Extensions </h4>

<p>
Of the possible advanced extension points, one additional point for this assignment can be earned by adding the capability of displaying laser scans from a real or simulated Fetch robot.
</p>

<p>
Of the possible advanced extension points, four additional points for this assignment can be earned by adding the capability of displaying 3D point clouds from a real or simulated Fetch robot and computing surface normals about each point.
</p>

<p>
Of the possible advanced extension points, four additional points for this assignment can be earned by implementing dynamical simulation through the recursive <a href="http://robotics.usc.edu/~aatrash/cs545/CS545_lecture_11_new.pdf">Newton-Euler algorithm</a> (Spong Ch.7).  This dynamical simulation update be implemented as function kineval.updateDynamicsNewtonEuler() in the file "kineval/kineval_controls.js".  In "home.html", the call to kineval.updateDynamicsNewtonEuler() should replace the call purely kinematic update in kineval.applyControls().
</p>

<p>
Of the possible advanced extension points, five additional points for this assignment can be earned by developing and implementing a maximal coordinate dynamical simulation of biped hopper with links as 3D rigid bodies, similar to those in "<a href="http://www.ai.mit.edu/projects/leglab/people/people.html">On The Run"</a> by Raibert and Hodgins at the <a href="http://www.ai.mit.edu/projects/leglab/robots/robots-main.html">MIT Leg Lab</a>. This maximal coordinate pendulum implementation should be contained within the subdirectory "hopper_3d" with an "index.html" file that can be open to execute the simulation.  <!-- Permission by the course staff must be granted first before attempting this advanced extension. -->
</p>

<h3> Project Submission</h3>
<p>
For turning in your assignment, push your updated code to the <b>master</b> branch in your repository.  
</p>


<p>


<!-- 
<hr>
<h1> Material beyond this point has not been assigned.  The descriptions below are unofficial and tentative.</h1>
<hr> 

-->

<!-- 
(uncomment marker end 6) -->


<p>

<!--
<br>
<br>
<br>
<hr>
<hr>
<hr>
<h1> Material beyond this point has not been assigned.  The descriptions below are unofficial and tentative.</h1>
<hr>
<hr>
<hr>
<br>
<br>
<br>
-->

<h2 id="assignment5">Assignment 5: Inverse Kinematics </h2>  
<p>
<b>Due 11:59pm, Monday, March 25, 2024</b>

<p>
Although effective, robot choreography in configuration space is super tedious and inefficient.  This difficulty is primarily due to posing each joint of the robot at each setpoint.  Further, changing one joint often requires updating several other joints due to the nature of kinematic dependencies.  Inverse kinematics (IK) offers a much easier and efficient alternative.  With IK implemented, we only need to pose the endeffector in a common workspace, and the states of the joints in configuration space are automatically inferred.  IK is also important when we care about the "tool tip" of an instrument being used by a robot.  One such example is a robot using marker to draw a picture, such as in the PR2 Portrait Bot Project below:
</p>

<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/qVS7oylkTwY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
</center>

<p>
For this assignment, you will now control your robot to reach to a given point in space through inverse kinematics for position control of the robot endeffector.  Inverse kinematics will be implemented through gradient descent optimization with both the Jacobian Transpose and Jacobian Pseudoinverse methods, although only one will be invoked at run-time.  
</p>

<img width=100% src="images/kineval_fetch.png">

<p>
As shown in the video below, if successful, your robot will be able to continually place its endeffector (indicated by the blue cube) exactly on the reachable target location (indicated by the green cube), regardless of the robot's specific configuration:
</p>

<center>
<iframe width="420" height="315" src="https://www.youtube.com/embed/ag0j8HzFRzc" frameborder="0" allowfullscreen></iframe>
</center>

<h3>Features Overview</h3>

<p>
  This assignment requires the following features to be implemented in the corresponding files in your repository:
</p>

<ul>
  <li> <p>Manipulator Jacobian in "kineval/kineval_inverse_kinematics.js"</p></li>
  <li> <p>Gradient descent with Jacobian transpose in "kineval/kineval_inverse_kinematics.js"</p></li>
  <li> <p>Jacobian pseudoinverse in "kineval/kineval_matrix.js" (pseudoinverse function) and "kineval/kineval_inverse_kinematics.js" (use in gradient descent)</p></li>
  <!-- <li> <p>[Grad section only] Euler angle conversion in "kineval/kineval_inverse_kinematics.js"</p></li> -->
</ul>

<p>
  Points distributions for these features can be found in the <a href="#project_rubric">project rubric section</a></td>. More details about each of these features and the implementation process are given below.
</p>


<!-- 
<!-- (uncomment marker begin 7, copy above to block uncomment or below to block comment)
  -->
<h3>Matrix Pseudoinverse Function</h3>
<p>
  You will need to implement one additional matrix helper function in "kineval/kineval_matrix.js" for this assignment: matrix_pseudoinverse. This method will be necessary for the pseudoinverse version of gradient descent (see below). For this helper function, you are allowed to use a library function for matrix inversion, which can be invoked by using the provided routine numeric.inv(mat), available through <a href="https://github.com/sloisel/numeric">numericjs</a>.
</p>

<h3>Core IK Function</h3>

<p>
The core of this assignment is to complete the kineval.iterateIK() function in the file kineval/kineval_inverse_kinematics.js. This function is invoked within the function kineval.inverseKinematics()  with three arguments:
</p>

<ul>
<li><p>endeffector_target_world: an object expressing the endeffector target in the world frame; it has two fields, endeffector_target_world.position, the target endeffector position (as a 3D homogeneous vector), and endeffector_target_world.orientation, the target endeffector orientation (as Euler angles)</p></li>
<li><p>endeffector_joint: the name of the joint directly connected to the endeffector</p></li>
<li><p>endeffector_position_local: the location of the endeffector in the local joint frame</p></li>
</ul>

<p>
From these arguments and the current robot configuration, the kineval.iterateIK() function will compute controls for each joint.  Upon update of the joints, these controls will move the configuration and endeffector of the robot closer to the target.
</p>

<p>
<b>Important:</b> Students in an undergraduate section are expected to implement inverse kinematics for <b>only</b> the position, not the orientation, of the endeffector.
</p>

<p>
kineval.iterateIK() should also respect global parameters for using the Jacobian pseudoinverse (through boolean parameter kineval.params.ik_pseudoinverse) and step length of the IK iteration (through real-valued parameter kineval.params.ik_steplength).  Note that these parameters can be changed through the user interface (under Inverse Kinematics). KinEval also maintains the current endeffector target information in the kineval.params.ik_target parameter.
</p>

<p>
IK iterations can be invoked through the user interface (Inverse Kinematics->persist_ik) or by holding down the 'p' key.  Further, the 'r'/'f' keys will move the target location up/down. You can also move the robot relative to the target using the robot base controls. When performing IK iterations, the endeffector and its target pose will be rendered as cube geometries in blue and green, respectively.
</p>

<p>
  For your code to work with the CI grader, you will need to set three global variables in kineval.iterateIK(): robot.dx, robot.jacobian, and robot.dq. There is a comment in "kineval/kineval_inverse_kinematics.js" that specifies what each of these variables should hold. Please note that robot.dx and robot.jacobian should both have six rows, even if you are doing position-only IK.
</p>

<p>
In implementing this IK routine, please also remember the following:
<p>
<ul>
<li><p>Computation of the Jacobian need only to occur with respect to the joints along the chain from the endeffector joint to the robot base</p></li>
<li><p>The location of the endeffector needs to be computed using transforms resulting from the robot's forward kinematics </p> </li>
<li><p>The computed velocity in configuration space should be applied to the robot through the .control field of each joint </p></li>
</ul>

<h3> IK Random Trial </h3>

<p>
All students in the AutoRob course are expected to run their IK controller with the random trial feature in the KinEval stencil.  The IK random trial is executed through the function kineval.randomizeIKtrial() in the file "kineval/kineval_inverse_kinematics.js".  This function is incomplete in the provided stencil.  Code for this function to properly run the random trial will be made available in the assignment 5 discussion channel. Once you have copied the necessary code into this function, you will be able to test your code on random trials by first selecting persist_ik (under Inverse Kinematics) then selecting execute (under Inverse Kinematics->IK Random Trial) from the user interface.
<p>

<!-- 
-->
<h3> Graduate Section Requirement</h3>

<p>
Students enrolled in the graduate section of AutoRob will implement inverse kinematics for both the position and orientation of the endeffector, namely for the Fetch robot.  The default IK behavior will be position-only endeffector control.  Both endeffector position and orientation should be controlled when the boolean parameter kineval.params.ik_orientation_included is set to true, which can be done through the user interface (Inverse Kinematics->ik_orientation_included).
</p>

<p>
  In order to handle the orientation of the endeffector in your IK implementation, you will need to calculate the orientation part of the error term, which will require you to implement a conversion from a rotation matrix to Euler angles. You may find an online reference to inform your implementation of this conversion (please cite it in a comment in your code) or develop your own approach to the conversion calculation. Completing this conversion is a necessary step for including orientation in your IK implementation, and it also fulfills the "Euler angle conversion" feature.
</p> 

<h4> Advanced Extensions </h4>

<p>
Of the possible advanced extension points, one extension point for this assignment can be earned by reaching to 100 targets in a random trial within 60 seconds.  A video of this execution must be provided to demonstrate this achievement.  This video file should be in the repository root directory with the name "IK100in60" and appropriate file extension.
</p>

<p>
Of the possible advanced extension points, three extension points for this assignment can be earned by implementing the <a href="http://ieeexplore.ieee.org/document/86079/">Cyclic Coordinate Descent (CCD)</a> inverse kinematics algorithm by Wang and Chen (1991).  This function should be implemented in the file "kineval/kineval_inverse_kinematics.js" as another option within the function kineval.iterateIK().
</p>

<p>
Of the possible advanced extension points, three extension points for this assignment can be earned by implementing <a href="https://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method">downhill simplex optimization</a> to perform inverse kinematics.  This function should be implemented in the file "kineval/kineval_inverse_kinematics.js" as another option within the function kineval.iterateIK().
</p>

<p>
Of the possible advanced extension points, one extension point for this assignment can be earned by implementing stochastic gradient descent to perform inverse kinematics.  This function should be implemented in the file "kineval/kineval_inverse_kinematics.js" as another option within the function kineval.iterateIK().
</p>


<p>
Of the possible advanced extension points, four extension points for this assignment can be earned by implementing resolved-rate inverse kinematics with null space constraints to respect joint limits.  This function should be implemented in the file "kineval/kineval_inverse_kinematics.js" as another option within the function kineval.iterateIK().
</p>

<p>
Of the possible advanced extension points, one extension point can be earned by implementing a closed-form inverse kinematics solution for the RexArm 4-DOF robot arm, which can be used later projects in <a href="https://www.youtube.com/playlist?list=PLDutmfAv2lfZ9M0XyYfY4N8EwLJhy58G6">EECS 467 (Autonomous Robotics Laboratory)</a>.  
</p>

<p>
Of the possible advanced extension points, four extension points for this assignment can be earned by extending your IK controller to use potential fields to avoid collisions.
</p>

<p>
Of the possible advanced extension points, one extension point for this assignment can be earned by implementing a search mechanism to automatically find appropriate PID gains for the Pendularm.  This implementation should be placed in the file "project_pendularm/pendularm1_gainsearch.html" and allow for arbitrary initial PID gains for the search to be set in the variable "initial_gains".
</p>


<h3> Project Submission</h3>

<p>
For turning in your assignment, ensure your completed project code has been committed and pushed to the <i>master</i> branch of your repository.  
</p>

<!-- (put this line below block comment end for block commenting, above for visibility
(comment marker end 7) -->


<!-- 
<hr>
<h1> Material beyond this point has not been assigned.  The descriptions below are unofficial and tentative.</h1>
<hr> 
-->
 
<!--
<br>
<br>
<br>
<hr>
<hr>
<hr>
<h1> Material beyond this point has not been assigned.  The descriptions below are unofficial and tentative.</h1>
<hr>
<hr>
<hr>
<br>
<br>
<br>
-->



<h2 id="assignment6">Assignment 6: Motion Planning</h2>
<p>
<b>Due 11:59pm, Monday, April 15, 2024</b>
</p>

<p>
Our last programming project for AutoRob returns to search algorithms for generating navigation setpoints, but now for a high-dimensional robot arm.  The A-star graph search algorithm in Assignment 1 is a good fit for path planning when the space to explore is limited to the two degrees-of-freedom of a robot base.  However, as the number of degrees-of-freedom of our robot increases, our search complexity will grow exponentially towards intractability. For such high-dimensional search problems, an exhaustive overview of the majority of the space is not an option. Instead, we now look to sampling-based search algorithms, which will introduce randomness to our search process. These sampling-based algorithms trade off the guarantees and optimality of exhaustive graph search for viably tractable planning in complex environments.  The example below shows one example of sampling-based planning navigating to move a rod through a narrow passageway:
</p>

<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/BPelkdxt1iU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
</center>

<p>
and such planning is also used in simple tabletop scenarios:
</p>

<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/ag-txw4KUgo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
</center>

<p>
  For this assignment, you will now implement a collision-free motion planner to enable your robot to navigate from a random configuration in the world to its home configuration (or "zero configuration").  This home configuration is where every robot DOF has a zero value.  For your planning implementation, the configuration space includes the state of each joint and the global orientation and position of the robot base.  Thus, the robot must move to its original state at the origin of the world. A visual explanation of this desired behavior is below:
</p>

<img src="images/asgn6_motionplan.png"  width=100%>

<p>
For both the undergraduate and graduate sections, motion planning will be implemented through the <a href="http://www.cs.cmu.edu/afs/cs/academic/class/15494-s12/readings/kuffner_icra2000.pdf">RRT-Connect algorithm</a> (described by Kuffner and LaValle). The graduate section will additionally implement the <a href="http://dspace.mit.edu/openaccess-disseminate/1721.1/63170">RRT-Star</a> (alternate paper <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5980479">link<a/> via IEEE) motion planner of Karaman et al. (ICRA 2011).
</p>

<h3>Features Overview</h3>

<p>
  This assignment requires the following features to be implemented in the corresponding files in your repository:
</p>

<ul>
  <li> <p>Collision detection in "kineval/kineval_collision.js"</p></li>
  <li> <p>2D RRT-Connect in "project_pathplan/rrt.js"</p></li>
  <li> <p>Configuration space RRT-Connect in "kineval/kineval_rrt_connect.js"</p></li>
  <!-- <li> <p>[Grad section only] 2D RRT-Star in "project_pathplan/rrt.js"</p></li> -->
</ul>

<p>
  Points distributions for these features can be found in the <a href="#project_rubric">project rubric section</a></td>. More details about each of these features and the implementation process are given below.
</p>

<!--
<!-- (uncomment marker begin 8)
-->
<h3>2D RRT-Connect</h3>

<p>
  To gain familiarity with the RRT-Connect algorithm, you can start this assignment by returning to the 2D world from Assignment 1. If needed, refer back to the Assignment 1 description for a description of the search canvas environment and its parameters. You can enable RRT-Connect as the search algorithm through the URL parameter search_alg: "search_canvas.html?search_alg=RRT-connect".
</p>

<p>
  You will implement the 2D version of RRT-Connect in project_pathplan/rrt.js by completing the iterateRRTConnect() function. Its signature and desired return values are provided in the code stencil. Note that there are other function stencils provided in this file as well, but your 2D RRT-Connect implementation should involve only iterateRRTConnect() and any helper functions you choose to add. You do <b>not</b> need to implement iterateRRT(), and only students in the graduate section need to implement iterateRRTStar() (see description of graduate section requirements below).
</p>

<p>
  A few other details to be aware of when implementing 2D RRT-Connect:
</p>

<ul>
  <li> <p>The two search tree global variables needed for RRT-Connect, T_a and T_b, are initialized for you in project_pathplan/infrastructure.js</p></li>
  <li> <p>You can use provided support functions from project_pathplan/infrastructure.js, including two new RRT-specific helpers: insertTreeVertex() and insertTreeEdge()</p></li>
  <li> <p>You may also create additional helper functions in project_pathplan/rrt.js to handle different steps of the RRT-Connect algorithm; some suggestions are provided in the code stencil</p></li>
  <li> <p>iterateRRTConnect() is called for you from the animation code, and it should perform just one iteration of the RRT-Connect algorithm each time it is called</p></li>
  <li> <p>You should use drawHighlightedPath(), <b>not</b> drawHighlightedPathGraph(), to visualize the final path found by RRT-Connect; see the implementation in draw.js for information about how this function works</p></li>
</ul>


<p>
  If properly implemented, your RRT-Connect implementation should produce results similar to the image below, although the inherent randomness of the algorithm will mean that the sampled states and final path will be slightly different:
</p>

<center>
<img src="images/rrt_connect.png"  width=600>
</center>

<h3>Getting Started in Configuration Space</h3>
<p>
The core of this assignment is to complete the robot_rrt_planner_init() and robot_rrt_planner_iterate() in kineval/kineval_rrt_connect.js. This file and the collision detection file kineval/kineval_collision.js have already been included in home.html for you:
</p>

<pre style="color:black"><code data-language="javascript">
    &lt;script src="kineval/kineval_rrt_connect.js">&lt/script>
    &lt;script src="kineval/kineval_collision.js">&lt/script>
</code></pre>

<p>
The code stencil will automatically load a default world.  A different world can be specified as an appended parameter within the URL: "home.html?world=worlds/world_name.js".
The world file specifies the global objects "robot_boundary", which describes the min and max values of the world along the X, Y, and Z axes, and "robot_obstacles", which contains the locations and radii of sphere obstacles.  To ensure the world is rendered in the display and available for collision detection, the geometries of the world are included through the provided call to kineval.initWorldPlanningScene() in kineval/kineval.js.
</p>


<h3>Collision Detection Setup</h3>
<p>
  In the search canvas world, a collision detection function was provided for you. For RRT-Connect in robot configuration space, you will need to start by completing the collision detection feature yourself. The main collision detection function used by configuration-space RRT-Connect is kineval.robotIsCollision() (in kineval/kineval_collision.js), which detects robot-world collisions with respect to a specified world geometry.
</p>

<p>
  Worlds are specified as a rectangular boundary and sphere obstacles.  A collection of worlds are provided in the "worlds/" subdirectory of kineval_stencil.  The collision detection system performs two forms of tests: 1) testing of the base position of the robot against the rectangular extents of the world, which is provided by default, and 2) testing of link geometries for a robot configuration against spherical objects, which depends on code you will write.
</p>

<p>
  Collision testing for links in a configuration should be performed by AABB/Sphere tests that require the bounding box of each link's geometry in the coordinates of that link. This bounding box is computed for you by the following code within the loop inside kineval.initRobotLinksGeoms() in kineval.js:
</p>

<pre style="color:black"><code data-language="javascript">
    // For collision detection,
    // set the bounding box of robot link in local link coordinates
    robot.links[x].bbox = new THREE.Box3;
    // setFromObject returns world space bbox
    robot.links[x].bbox = robot.links[x].bbox.setFromObject(robot.links[x].geom);
  </code>
</pre>

<p>
  As you write the collision test, you can thus access the AABB for any robot link as robot.links[x].bbox. This object contains two elements, max and min, that contain the maximum and minimum corners of the link's bounding box, specified in the link's local coordinate frame.
</p>

<!--
(end comment marker 8) --> 
<!-- -->

<p>
  Even before your planner is implemented, you can use the collision system interactively with your robot.  The provided kineval.robotIsCollision() function is called for you during each iteration from my_animate() in home.html:
</p>

<pre style="color:black"><code data-language="javascript">
    // determine if robot is currently in collision with world
    kineval.robotIsCollision();
</code></pre>

<!-- 
<!-- (uncomment marker begin 9)
-->

<h3>Completing Collision Detection</h3>
<p>

To complete the collision system, you will need to modify the forward kinematics calls in kineval/kineval_collision.js.  Specifically, you will need to perform a traversal of the forward kinematics of the robot for an arbitrary robot configuration within the function kineval.poseIsCollision().  kineval.poseIsCollision() takes in a vector in the robot's configuration space and returns either a boolean false for no detected collision or a string with the name of a link that is in collision.  As a default, this function performs base collision detection against the extents of the world.  For collision detection of each link, this function will make a call to function that you create called robot_collision_forward_kinematics() to recursively test for collisions along each link.  Your collision FK recursion should use the link collision function, traverse_collision_forward_kinematics_link(), which is provided in kineval/kineval_collision.js, along with a joint traversal function that properly positions the link and joint frames for the given configuration.
</p>

<!--
<pre style="color:black"><code data-language="javascript">
function collision_FK_link(link,mstack,q) {

  // this function is part of an FK recursion to test each link 
  //   for collisions, along with a joint traversal function for
  //   the input robot configuration q
  //
  // this function returns the name of a robot link in collision
  //   or false if all its kinematic descendants are not in collision

  // test collision by transforming obstacles in world to link space
  mstack_inv = numeric.inv(mstack);
  // (alternatively) mstack_inv = matrix_invert_affine(mstack);

  var i; var j;

  // test each obstacle against link bbox geometry 
  //   by transforming obstacle into link frame and 
  //   testing against axis aligned bounding box
  for (j in robot_obstacles) {

    var obstacle_local = 
      matrix_multiply(mstack_inv,robot_obstacles[j].location);

    // assume link is in collision as default
    var in_collision = true;

    // return false if no collision is detected such that
    //   obstacle lies outside the link extents 
    //   along any dimension of its bounding box
    if (
      (obstacle_local[0][0]<
       (link.bbox.min.x-robot_obstacles[j].radius)
      )
      ||
      (obstacle_local[0][0]>
       (link.bbox.max.x+robot_obstacles[j].radius)
      )
    )
      in_collision = false;

    if (
      (obstacle_local[1][0]<
       (link.bbox.min.y-robot_obstacles[j].radius)
      )
      ||
      (obstacle_local[1][0]>
       (link.bbox.max.y+robot_obstacles[j].radius)
      )
    )
      in_collision = false;

    if (
      (obstacle_local[2][0]<
       (link.bbox.min.z-robot_obstacles[j].radius)
      )
      ||
      (obstacle_local[2][0]>
       (link.bbox.max.z+robot_obstacles[j].radius)
      )
    )
      in_collision = false;

    // return name of link for detected collision if
    //   obstacle lies within the link extents 
    //   along all dimensions of its bounding box
    if (in_collision)
      return link.name;
  }

  // recurse child joints for collisions, 
  //   returning name of descendant link in collision
  //   or false if all descendants are not in collision
  if (typeof link.children !== 'undefined') { 
    var local_collision;
    for (i in link.children) {
       // STUDENT: create this joint FK traversal function 
       local_collision = 
         collision_FK_joint(robot.joints[link.children[i]],mstack,q)
       if (local_collision)
         return local_collision;
     }
  }

  // return false, when no collision detected for this link and children
  return false;
}
</code></pre>


<p>
kineval_collision.js uses matrix and quaternion calls based on the reference implementation (i.e., the instrutor's code).  Your matrix and quaternion calls likely have a different structure to the function arguments and returned data structures.  You should either:
</p>

 <ul>
  <li><p>modify calls to matrix/quaternion routines to fit your functions, or</p></li>

  <li><p> use a modified version of your own FK with the collision test added to the link traversal function (remember: you need the inverse of the matrix stack for collision testing in a link frame)</p></li>
</ul>
<p>
  -->
<p>
  Some pointers about your collision FK traversal:
</p>

<ul>
  <li> <p>Remember that you need the inverse of the matrix stack for collision testing in a link frame (instead of in the world frame)</p></li>
  <li> <p>You can feel free to implement matrix_invert_affine() instead of using numeric.inv().  Affine transforms can be inverted (in constant time, Quiz 3!) through a much simpler process than the generic matrix inversion, which is O(n^3) for Gaussian elimination.</p></li>
</ul>

<p>
If successful to this point, you should be able to move the robot around the world and see the colliding link display a red wireframe when a collision occurs. There could be many links in collision, but only one will be highlighted, as shown in the following examples:
</p>

<p>
<center>
<img src="images/rrt_collision_boundary.png" width=48%>
<img src="images/rrt_collision_link.png" width=48%>
</center>
</p>



<h3>Implementing and Invoking the Planner</h3>

<p>
  Your motion planner will be implemented in the file kineval/kineval_rrt_connect.js through the functions kineval.robotRRTPlannerInit() and robot_rrt_planner_iterate(). This implementation can be a port of your 2D RRT-Connect, but it will require some updates to work with in the configuration space of KinEval robots. The kineval.robotRRTPlannerInit() function should be modified to initialize the RRT trees and other necessary variables. The robot_rrt_planner_iterate() function should be modified to perform a <b>single</b> RRT-Connect iteration based on the current RRT trees.
</p>

<p>
  Basic RRT tree support functions are provided for initialization, adding configuration vertices (which renders "breadcrumb" indicators of base positions explored), and adding graph edges between configuration vertices.  This function should <b>not</b> use a for loop to perform multiple planning iterations, as this will cause the browser to block and become unresponsive.  Instead, the planner will be continually called asynchronously by the code stencil until a motion plan solution is found.
</p>

<p>
<b>Important:</b> Your planner should be constrained such that the search does not consider configurations where the base is outside the X-Z plane.  Specifically, the base should not translate along the Y axis, and should not rotate about the X and Z axes.
</p>

<p>
Once implemented, your planner will be invoked interactively by first moving the robot to an arbitrary non-colliding configuration in the world and then pressing the "m" key.  The "m" key will request the generation of a motion plan. The goal of a motion plan will always be the home configuration, as defined in the introduction to this assignment.  While the planner is working, it will not accept new planning requests.  Thus, you can move the robot around while the planner is executing.
</p>


<h3>Planner Output</h3>
<p>
The output of your planner will be a motion path in a sequentially ordered array (named kineval.motion_plan[]) of RRT vertices.  Each element of this array contains a reference to an RRT vertex with a robot configuration (.vertex), an array of edges (.edges), and a threejs indicator geometry (.geom).  Once a viable motion plan is found, this path can be highlighted by changing the color of the RRT vertex "breadcrumb" geom indicators.  The color of any configuration breadcrumb indicator in a tree can be modified, such as in the following example for red:
</p>

<pre style="color:black"><code data-language="javascript">
  tree.vertices[i].geom.material.color = {r:1,g:0,b:0};
</code></pre>

<p>
The user should should be able to interactively move the robot through the found plan.  Stencil code in user_input() within kineval_userinput.js will enable the "n" and "b" keys to move the robot to the next and previous configuration in the found path, respectively.  These user key presses will respectively increment and decrement the parameter kineval.motion_plan_traversal_index such that the robot's current configuration will become:
</p>

<pre style="color:black"><code data-language="javascript">
  kineval.motion_plan[kineval.motion_plan_traversal_index]
</code></pre>

<p>
<b>Note:</b> we are <b>NOT</b> using robot.joints[...].control to execute the found path of the robot.  Although this can be done, the collision system does not currently test for configurations that occur due to the motion between configurations.
</p>

<p>
  The result of your RRT-Connect implementation in configuration space should look similar to this path found in the worlds/world_s.js world:
</p>
<img src="images/asgn6_scurve_2016.png" width=100%>

<h3>Testing</h3>

<p>
Make sure to test all provided robot descriptions from a reasonable set of initial configurations within all of the provided worlds, ensuring that:
</p>

<p>
<ul>
<li> <p>a valid non-colliding path is found and can be traversed,</p></li>
<li> <p>the robot does not to take steps longer than 1 unit,</p></li>
<li> <p>the robot base does not move outside the X-Z plane.  Specifically, the base should not translate along the Y axis, and should not rotate about the X and Z axes.</p></li>
</ul>
</p>


<h3>Warning: Respect Configuration Space</h3>

<p>
The planner should produce a collision-free path in configuration space (over all robot DOFs) and not just the movement of the base on the ground plane.  If your planner does not work in configuration space, it is sure to fail tests used for grading.
</p>

<!-- 
-->
<h3> Graduate Section Requirement</h3>

<p>
In addition to the requirements above, students in the graduate section must also implement the <a href="http://dspace.mit.edu/openaccess-disseminate/1721.1/63170">RRT-Star</a> motion planning algorithm for the 2D search canvas. You will need to complete the iterateRRTStar() function stencil in project_pathplan/rrt.js for this feature. Part of this assignment is an exercise in how to conceptualize implementation details of an algorithm from a robotics paper. Because of this, you will need to refer to the linked paper for details on how to implement the RRT-Star algorithm. <b>Note:</b> The course staff will not provide assistance with RRT-Star, so we strongly encourage high-level discussion of the algorithm among students on the assignment channel and amongst peers.
</p> 

<!--
<p>
<ul>
<li> <p>joint limits for the different joint types are respected, and </p></li>
<li> <p>the "fetch" robot should be able to navigate all of the provided worlds.</p></li> 
</ul>
</p>
-->

<h4> Advanced Extensions</h4>

<p>
Of the possible advanced extension points, one additional point for this assignment can be earned by adding the capability of motion planning to an arbitrary robot configuration goal. 
</p>
<p>
Of the possible advanced extension points, two additional points for this assignment can be earned by using the A-star algorithm for base path planning in combination with RRT-Connect for arm motion planning. 
</p>
<p>
Of the possible advanced extension points, one additional point for this assignment can be earned by writing a collision detection system for two arbitrary triangles in 2D using a JavaScript/HTML5 canvas element. 
</p>
<p>
Of the possible advanced extension points, two additional points for this assignment can be earned by writing a collision detection system for two arbitrary triangles in 3D using JavaScript/HTML5 and threejs or a canvas element. 
</p>
<p>
Of the possible advanced extension points, four additional points for this assignment can be earned by implementation of triangle-triangle tests for collision detection between robot and planning scene meshes. 
</p>
<p>
Of the possible advanced extension points, three additional points for this assignment can be earned by implementation of cubic or quintic polynomial interpolation (Spong Ch. 5.5.1 and 5.5.2) across configurations returned in a computed motion plan.
</p>
<p>
Of the possible advanced extension points, four additional points for this assignment can be earned by implementing an approved research paper describing a motion planning algorithm. 
</p>




<!--
<h3>Highly recommended: start with HTML5 Canvas Stencil </h3>

<img width=100% src="images/asgn6_rrt_canvas_stencil_small.png">

<p>
Using the browser for as a development environment has many benefits.  However, when coding mistakes occur, it will make the browser lock up and be completely unusable.  Such mistakes can be especially difficult to debug when the overhead of rendering with threejs is involved.  
</p>

<p>
To help you get started, the path planning code stencil in the "search_canvas" directory has entry points for developing your core RRT routines.  This stencil will allow you to implement the RRT-Connect algorithm in simplified 2D worlds with provided routines for visualization and collision.  Because the RRT is invariant across configuration spaces, an RRT developed for the 2D Canvas world should easily port to the N-D threejs world, with minor changes for invoking drawing routines.
</p>

<p>
  -->

<h3>Project Submission</h3>
<p>
For turning in your assignment, ensure your completed project code has been committed and pushed to the <i>master</i> branch of your repository.  
<p>

<!--
(comment marker end 9) -->

<h2 id="assignment7">Assignment 7: The best use of robotics? </h2>  
<p>

<b>Slides due 11:59pm, Friday, April 19, 2024</b><br>
<b>Presentation due 4:30pm, Monday, April 22, 2024</b>

<!-- <b><s>Video Presentation due 11:59pm, Friday, December 4, 2020</s> Cancelled</b><br> -->
<p>

Scenario: An investor is considering giving you 20 million dollars (cold hard USD cash, figuratively).  This investor has been impressed by your work with KinEval and other accomplishments while at the University of Michigan.  They are convinced you have the technical ability to make a compelling robot technology... but, they are unsure how this technology could produce something useful.  Your task is to make a convincing pitch for a robotics project that would yield a high return on investment, as measured by some metric (financial profit, good for society, creation of new knowledge, etc.). 

<!-- 
<!-- (uncomment marker begin 10)
-->
<p>
You will get 60 seconds to make a pitch to develop something useful with robots.  Consider the instructor and your classmates as the people that need to be convinced.  As a guideline, your pitch should address an opportunity (presented by a need or a problem), your planned result (as a system, technology, product, and/or service), and how you will measure successful return on investment.  Return on investment can be viewed as financial profit (wrt. venture capital), good for society (wrt. a government program), creation of new knowledge or capabilities (wrt. a grant for scientific research).  Remember, the purpose is to convince and inspire about what is possible, rather than dive into specifics.
<p>
The last scheduled class period (April 22, 4:30-7:30pm) will be dedicated to student presentations during which each student will have 60 seconds to make their pitch to the rest of class.  
</p>

<p>
Please insert your finalized presentation slides in the Best Use of Robotics Google Slides presentation before 11:59pm on Friday April 19th.  A link to this Google Slides presentation will be provided via the course Slack workspace and Piazza discussion board.  Your first slide must include the title of your presentation, your name, and your U-M Michigan unique name.
<!--  The filename of your slides must start with your username in the format "asgn7_username".  Slides will only be accepted in PDF format, although embedding of videos or links to videos will be accepted.  You can post new versions of your slides up to the submission deadline on December 6th, but must delete older versions.
-->
</p>
<!--
(end comment marker 10) -->
<!-- -->
<p>
The pitch judged to be the most convincing will get first dibs.
</p>

<!--
<hr>
<h1> Assignments beyond this point have not been assigned.  The descriptions below are unofficial and tentative.</h1>
<hr>
-->




<!-- moved to course missive in google docs
    
<p>
<br>
<br>
<hr>
<hr>
<h1>Additional Materials</h1>


<h2 id="faq">Frequently Asked Questions</h2>


<ul>
<li><p><a href="#meetings">When does the course meet?</p></a>
<li><p><a href="#grading">How many projects are there and how are they graded?</p></a>
<li><p><a href="#schedule">What is the schedule for topics and projects?</p></a>
</ul>


<h2>AutoRob Course Learning Objectives <id="objectives"></h2>

<p>
AutoRob focuses on the computational foundations for kinematic modeling and planning for autonomous robots with an emphasis on manipulation and mobility.  Successful completion of AutoRob will result in the student having implemented a full software stack for "mobile pick-and-place."  That is, given a robot and perception (or "full observation") of the robot's environment, the resulting code modules can enable the robot to pick up an object at an arbitrary location and place the object in a new location.

At a finer level of resolution, a student who has successfully completed the AutoRob course in Winter 2023 is capable of the following:
</p>

<ol>
<p>
<li><p> Explain how a publish-subscribe messaging model works for robot middleware systems </p></li>
<li><p> Implement collision-free 2D path planning using the A-Star algorithm in a procedural computer programming language </p></li>
<li><p> Formulate a graph and a search procedurel to find an optimal route to a given goal location</p></li>
<li><p> Implement dynamical simulation for a robot through numerical integration given equations of motion </p></li>
<li><p> Create a kinematic description of an arbitrary open-chain robot in the Universal Robot Description Format (URDF) </p></li>
<li><p> Compute solutions for the gripper on an arbitrary robot arm to reach an arbitrary goal location in a 3D workspace through forward and inverse kinematics </p></li>
<li><p> Define mathematically how two 3D homogeneous transforms can be composed into a common frame of refrence </p></li>
<li><p> State mathematically the effect of the movement of a robot's joint on the pose of the robot's endeffector </p></li>
<li><p> Compose a gradient descent optimization algorithm to search for parameters that minimize error expressed as a function </p></li>
<li><p> Implement collision-free high-dimensional motion planning using the RRT-Connect algorithm in configuration space </p></li>
</ol>




<h2 id="git_tutorial">Git-ing Started with Git</h2>

<p>
Each student in this class is responsible for providing a git repository for submitting their project work and receiving grading feedback.  Use of the GitHub classroom is available as a complementary service that provides repositories free for student use.  If a student is uncomfortable using the GitHub service, the <a href="https://gitlab.umich.edu/">EECS GitLab Server</a> is a service within the University of Michigan and is available for creation of student repositories.
</p>

<p>
Using version control effectively is an essential skill for both the AutoRob course and, more generally, contributing to advanced projects in robotics research and development.  Git is arguably the most widely used version control system at the current time.  Examples of the many robotics projects using Git include:
<a href="https://github.com/lcm-proj">Lightweight Communications and Marshalling</a>,
<a href="https://github.com/ros">the Robot Operating System</a>,
<a href="https://github.com/RobotWebTools">Robot Web Tools</a>,
<a href="https://github.com/fetchrobotics">Fetch Robotics</a>, and
<a href="https://github.com/RethinkRobotics">the Rethink Baxter</a>.
To help you use Git effectively, the course staff has added the tutorials below for getting started with Git.
This is meant to be a starting guide to using Git version control and the bash command shell.  For a more complete list of commands and features of Git, you can refer to the following guides: <a href="http://git-scm.com/book/en/v2/Getting-Started-Installing-Git">The Git Pro book</a> or The <a href="http://johnatten.com/2012/09/08/basic-git-command-line-reference-for-windows-users/">Basic Git command line reference for windows users</a>. An interactive tutorial for Git is available at <a href="http://learngitbranching.js.org/">LearnGitBranching</a>.
</p>


<h3>Installing Git</h3>

<p>
The AutoRob course assumes Git is used from a command line terminal to work with the a Git hosting service, such as <a href="https://github.com">GitHub</a>, <a href="https://bitbucket.com">Bitbucket</a>, or <a href="https://gitlab.eecs.umich.edu/">EECS GitLab</a>.  Such terminal environments are readily available in Linux and Mac OSX through their respective terminal programs.  For MS Windows, we recommend Git Bash, which can be downloaded from the <a href="https://gitforwindows.org/">Git for Windows</a> project.  Several other viable alternatives Git clients exist, such as the GUI-based <a href="https://www.gitkraken.com/">GitKraken</a>.  
</p>

<p>
Git can be installed on Linux through the package managment system used by your Linux distro, likely with one of the following commands:
</p>

<pre style="color:black"><code data-language="javascript">
    sudo yum install git-all
</code></pre>
<pre style="color:black"><code data-language="javascript">
    sudo apt install git
</code></pre>

<p>
For Mac OSX, Git can be installed on its own using the <a href="https://sourceforge.net/projects/git-osx-installer/">Git-OSX-Installer</a> or as part of larger set of <a href="https://en.wikipedia.org/wiki/Xcode">Xcode</a> build tools. 
</p>

<p>
If you open the "Git Bash" program on Windows or the "Terminal" program on Mac OSX or Linux, you should see a shell environment that looks something like this (screenshot from an older version of Windows Git Bash):
</p>

<center>
<img width=80% src="images/gitbash.png">
</center>

<p>
If you have Git installed, you should should be able to enter the "git" command and see the following usage information printed (screenshot from OSX):
</p>

<center>
<img width=80% src="images/git_terminal_osx.png">
</center>




<h3>GitHub Classroom</h3>

<p>
If you choose to use GitHub for hosting your repository, the course staff has created a GitHub classroom which students can choose to use for setting up their repository. If a student is uncomfortable using the GitHub service, the <a href="https://gitlab.umich.edu/">EECS GitLab Server</a> is a service within the University of Michigan and is available for creation of student repositories.
</p>

<p>
Once you have configured Git with your local development environment, you should then be able to join the autorob-WN22 GitHub classroom by following the instructions at this <a href="https://classroom.github.com/a/0ieHZNvR">invitation link</a>. This link should take you to the following page:
</p>

<center>
<img width=80% src="images/github_classroom_invite.png">
</center>

<p>
By clicking on 'Skip to the next step', shown above, you will be able to accept the invitation. After accepting, you will be enrolled in the autorob-WN23 classroom and a private clone of the <a href="https://github.com/lizolson/kineval-stencil">KinEval stencil repository</a> will be created for you to use. Your private repository will automatically be named <code>kineval-stencil-&ltusername&gt</code>. After accepting, you should see a page similar to the one below:
</p>

<center>
<img width=80% src="images/github_classroom_result.png">
</center>

<p>
<b>If you have any issues or questions about setting up your repository, contact the course staff through slack or email for help.</b>
</p>

<h3>Cloning your repository</h3>

<p>
The most common thing that you will need to do is pull and push files from and to your Git hosting service. Upon opening Git Bash or the terminal, you will need to go to the location of <b>both</b> your GitHub/Bitbucket/GitLab repository on the web and your Git workspace on your local computer.  Our first main step is to clone your remote repository onto your local computer.  Towards this end, the next step is to determine your current directory, assuming you will use this directory to create a workspace.  For Linux and OSX, the terminal should start in your home directory, often "/home/username" or "/Users/username".  For Git Bash on Windows, the default home directory location could be the Documents in your user directory, or the general user folder within "C:\Users".
</p>

<p>
From your current directory, you can use Bash commands to view and modify the contents of directories and files.  You can see a list of the files and folders that can be accessed using ls (list) and change the folder using the command cd (change directory) as shown below. If you believe that the directory has files in addition to folders, but would like a list of just the folders, then the command ls â€“d */ can be used instead of ls.  Below is a quick summary of relevant Bash commands (or reference the <a href="https://files.fosswire.com/2007/08/fwunixref.pdf">cheat sheet here</a>):
</p>

<p>
<ul>
<li>"ls" prints a listing of files in the current directory
<li>"pwd" prints the location of the current directory in the filesystem
<li>"cd [FolderName]" moves the terminal to a new directory in the filesystem
<li>"ls [Expression]" prints a listing of files in the current directory matching the given Expression; ls r* prints all files starting with the character 'r'
<li>"mkdir [FolderName]" creates a folder within the current directory. If the folder name has spaces, then NameFolder will need to be in double quotes.
<li>"rmdir [FolderName]" removes a specified empty folder. If it is not empty, the folder will not be removed.
<li>"rm â€“rf [FolderName]" removes a specified folder and all the contents
<li>"touch [FileName]" creates a single empty text file. Note: file names cannot have spaces!
<li>"touch [FIleName1.txt] [FileName2.txt]..." creates multiple empty text files
<li>"rm [FileName]" removes a specific file from the current directory
<li>"rm â€“i [FileName]" confirmation prompt required before removing file from current directory.
<li>"rm â€“v [FileName]" removes the file and reports in console
</ul>
</p>

<p>
Once you have navigated to the directory where you want to create your workspace, you are ready to clone a copy of your remote repository and populate it with files for AutoRob projects.  It assumed that you have already created a repository on your Git hosting service, given the course staff access to this repository, and provided a link of your repository to the course staff.  You will need the repository link in the form of "https://github.com/autorob-WN23/kineval-stencil-&ltusername&gt" if you are using HTTPS (default) or 
    "git@github.com:autorob-WN22/kineval-stencil-&ltusername&gt.git" if you are using SSH (only if you have set up your SSH keys). You'll use this link to clone a copy of your remote repository onto your local machine using the following git command below.  This command will clone the repository contents to a subdirectory labeled with the name of the repository:
</p>

<pre style="color:black"><code data-language="javascript">
    git clone [repository URL link]
</code></pre>

<p>
This directory should be listed and inspected to ensure it has been cloned with the contents of the repository, matching the remote repository from your Git hosting service.  If this is a new repository, it is not problem for this directory to be empty:
</p>

<pre style="color:black"><code data-language="javascript">
    ls [repository_name]
</code></pre>

<p>
After cloning has finished, you can also check for differences between the files on your computer and the remote repository by running the "git status" command in from within your newly-created directory as shown below.  If you receive the message shown in the example below, then there are no differences. If there are differences, then it will have the number of files which are different highlighted in red.
</p>

<pre style="color:black"><code data-language="javascript">
    $ git status
    On branch master
    Your branch is up-to-date with 'origin/master'.
    nothing to commit, working directory clean
</code></pre>

<h3>Important: workspace is not the same as repository</h3>

<p>
You should now have a local copy of your repository.  It is critical to note that your local repository is different than your current workspace.  Your workspace is not automatically tracked by the version control system and considered ephemeral.  Any changes made to your workspace must be committed into the local repository to be recognized by the version control system.  Further, any changes committed to your local repository must also be pushed remotely to be recognized by your Git hosting service.  Thus, any changes made to your workspace can be lost if not committed and pushed, which will be discussed more in later sections.
</p>

!-- <h3>Populating your repository with project stencil code</h3>

<p>
Use the "git remote" command to add a second remote repostiory to your new local repository:
</p>

<pre style="color:black"><code data-language="javascript">
    cd [repository_name]
    git remote add upstream https://github.com/autorob/kineval-stencil.git
</code></pre>

<p>
Now pull the files from the autorob remote with the following command:
</p>

<pre style="color:black"><code data-language="javascript">
    git pull upstream master
</code></pre>

<p>
  Note: The above command may not work if your local repository has preexisting commits in it. ONLY if you are having trouble with the pull command, you can try the following command instead:
</p>

<pre style="color:black"><code data-language="javascript">
    git reset --hard upstream/master
</code></pre>
<p>
  The "reset --hard" command will erase files and history, so be extra careful that you are not overwriting an important repository here!
</p> ->

<h3>Testing out the stencil code</h3>

<p>
Your folder should now be populated with the KinEval files. Open "home.html" in a web browser and ensure you see the starting point page pictured below:
<!-
for <a href="#assignment1">Assignment 1</a>.
->
</p>

<img width=100% src="images/kineval_welcome.png">

<p>
If your browser throws an error when loading "home.html", one potential cause is that this browser disallows loading of local files.  In such cases, the browser will typically report a security error in the console.  This security issue is avoided by serving the KinEval files from an HTTP server.  Such a HTTP server is commonly available within distributions for modern operating systems.  Assuming Python is installed on your computer, you can start a HTTP with the following command from your workspace directory, and then view the page at <a href="http://localhost:8000/home.html">localhost:8000</a>:
</p>

<pre style="color:black"><code data-language="javascript">
    python -m SimpleHTTPServer
</code></pre>

<p>
Alternatively, if you have nodejs installed, you can install and start a HTTP with the following command from your workspace directory, and then view the page at <a href="http://localhost:3000/home.html">localhost:3000</a>:
</p>

<pre style="color:black"><code data-language="javascript">
    npm install simple-server
    node simple-server
</code></pre>

<h3>Commit and push to update your repository</h3>

<p>
Whenever you make any significant changes to your repository, these changes should be committed to your local repository and pushed to your remote repository.  Such changes can involve adding new files or modifying existing files in your local workspace.  For such changes, you will first commit changes from your workspace to your local repository using the "git add" then "git commit" commands:
</p>

<pre style="color:black"><code data-language="javascript">
    git add [FileName]
    git commit -m "message describing changes"
</code></pre>

<p>
and then pushing these changes from your local repository to a synced repository on your git hosting service:
</p>

<pre style="color:black"><code data-language="javascript">
    git push
</code></pre>

<p>
This commit will occur to the "master" branch of your repository.
</p>

<p>
Note: the change files must be located in the correct repository folder on your local computer and these commands should be performed in the local workspace directory.  Below is a more detailed summary of git commands for adding files from your workspace to your repository:
</p>

<ul>
<li>"git add" adds changed files to the next commit. There are several different options which can follow this command.
<li>"git add â€“A" adds all new files and changes to the next commit including deletions (not recommended)
<li>"git add â€“u" adds all changes to the next commit without new files
<li>"git add [FileName]" add all changes to a specific file to the next commit
<li>"git commit" commits files that have been staged with a "git add" command. A commit message (specified with "-m") is required and is good practice to list changes that you have made.
<li>"git commit â€“m "Message"" commits all files staged with "git add"
<li>"git push" pushes the committed changes to remote repository
</ul>

<p>
  Note: If you are unsure about the options to use with these commands or any other git command, "-h" is your friend. Try the following commands:
</p>

<pre style="color:black"><code data-language="javascript">
    git add -h
    git commit -h
    git push -h
</code></pre>

<p>
Once you have committed and pushed, your changes have been safely stored and tracked remotely.  The local workspace <i>could</i> now be deleted without concern.  This local workspace can also be updated with changes to the remote repository by pulling.
</p>

<h3>Pulling remote changes</h3>

<p>
Changes can be made to your remote repository, potentially by other collaborators, without being continuously tracked by your local repository.  This can lead to potential versioning conflicts when committed changes contradict each other.  For the AutoRob course, versioning conflicts should not be a problem because commits to your repository, other than those from the course staff, should be yours alone.  That said, one good practice is to ensure your workspace, local repository, and remote repository are synced before making any changes.  A brute force method for doing this is to re-clone your repository each time you begin to make changes.  Another option is to pull remote changes into your local repository and workspace using the git pull command:
</p>

<pre style="color:black"><code data-language="javascript">
    cd [repository_name]
    git pull
</code></pre>

<p>
Below is a more detailed summary of git commands for pulling and fetching:
</p>

<ul>
<li>"git pull [RemoteName]" is used for retrieving commits and merging the files into what is already in the local workspace. This may make changes to the files that are already there; effectively, this is a fetch followed by a merge. You do not need the [RemoteName] parameter if you are pulling from your default remote
<li>"git fetch [RemoteName]" is used for retrieving commits from a remote repository <i>without</i> merging them into the workspace. This creates a copy of the updates in your local repository but does not apply them to the workspace.
</ul>

<!-
Some easy options for controlling previous and current versions of files can be done through the following commands.
git reset -- hard undo everything from last commit
git reset â€“ hard ORIG_HEAD undo last successful merge and following changes
git reset â€“soft HEAD^ undo most recent commit
git reset HEAD [FileName] remove a file from the next commit
->

<h3>Branching</h3>

<p>
Branching is an effective mechanism for work in a repository to be done in parallel with changes merged at a later point.  A branch essentially creates a copy of your work at a particular version.  Branches are independently tracked by the version controller and can be merged together when requested (which can result in a "pull request" when you're working on a repository in collaboration with others).  The larger story for branching and merging is outside the scope of AutoRob.
</p>

<p>
The working version of your code, which you submit for grading, is expected to be in the <i>master</i> branch of your repository.  When working on a new assignment, it is recommend that you create a branch for this new work.  This allows your stable code in the <i>master</i> branch to be undisturbed while you continue to modify your code.  Once your work for this assignment is done, you can then update your <i>master</i> by merging in your assignment branch.  Stylistically, it is helpful to use a branch name like <i>Assignment-X</i> for your assignment branch for project number <i>X</i>.
</p>

<p>
The simplest means for branching in this context is to use the branching feature from the webpage of your remote repository.  From GitHub, simply select the master branch from the "Branch: " button and enter the name of the branch to be created.  From Bitbucket, select the "Branches" icon from the left hand toolbar and follow the instructions for branch creation.  If successful, you should see a list of branches that can each be inspected for their respective contents.  Branches can also be deleted from this interface. You will need to pull from your remote repository after creating any branches from this interface to see them in your local repository.
</p>

<p>
A branch can also be created from the command line by the following, which will create a copy of the current branch:
</p>

<pre style="color:black"><code data-language="javascript">
    git branch [branch_name]
</code></pre>

<p>
You can switch between branches with the following command:
</p>

<pre style="color:black"><code data-language="javascript">
    git checkout [branch_name]
</code></pre>

<p>
as well as clone a specific branch from a repository:
</p>

<pre style="color:black"><code data-language="javascript">
    git clone -b [branch_name] [repository URL link]
</code></pre>

    -->
<p>
Good luck and happy hacking!
</p>

<br>
<br>

<img width=100% src="images/um_progress_fetch.jpeg">


        <!-- TYPEKIT -->
        <script type="text/javascript" src="resources/ajf8ggy.js"></script>
        <script type="text/javascript">try{Typekit.load();}catch(e){}</script>
        <script type="text/javascript" src="resources/rainbow_github.min.js"></script>
        <script type="text/javascript" src="resources/rainbow_github_generic.js"></script>
<!--
        <script type="text/javascript" src="//use.typekit.net/ajf8ggy.js"></script>
        <script type="text/javascript">try{Typekit.load();}catch(e){}</script>
        <script src="https://rawgithub.com/ccampbell/rainbow/master/js/rainbow.min.js"></script>
        <script src="https://rawgithub.com/ccampbell/rainbow/master/js/language/generic.js"></script>
-->

</body>

</html>

